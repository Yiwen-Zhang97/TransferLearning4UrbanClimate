{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf22137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms \n",
    "import torchvision.models as models\n",
    "import os\n",
    "import random\n",
    "\n",
    "PATH_TO_DATA =  \"/mnt/analysis/analysis/rand_sharded_data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aff9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_normalize = transforms.Normalize(\n",
    "                  mean=[0, 0, 0, 0, 4.0999e+02],\n",
    "                  std=[1, 1, 1, 1, 7.4237e+01]\n",
    ")\n",
    "#with clipping to 600: 4.0999e+02, 7.4237e+01\n",
    "#without clipping: 4.1454e+02, 8.9422e+01\n",
    "\n",
    "forcing_normalize = transforms.Normalize(\n",
    "                  mean=[444.9605606256559, 991.7980623653417, 0.00039606951184754176, 96111.04161525163, 0.006652783216819315, 314.3219695851273, 2.82168247768119],\n",
    "                  std=[5.5216369223813535, 12.951212256256913, 0.0002824274832735609, 975.3770569179914, 0.00012386107613000674, 0.6004463118907452, 0.34279194598853185]\n",
    ")\n",
    "\n",
    "elevation_mean = torch.from_numpy(np.array([4.0999e+02]))\n",
    "elevation_std = torch.from_numpy(np.array([7.4237e+01]))\n",
    "\n",
    "forcing_mean = torch.from_numpy(np.array([444.9605606256559, 991.7980623653417, 0.00039606951184754176, 96111.04161525163, 0.006652783216819315, 314.3219695851273, 2.82168247768119]))\n",
    "forcing_std = torch.from_numpy(np.array([5.5216369223813535, 12.951212256256913, 0.0002824274832735609, 975.3770569179914, 0.00012386107613000674, 0.6004463118907452, 0.34279194598853185]))\n",
    "\n",
    "lst_mean = torch.from_numpy(np.array([312.8291360088677]))\n",
    "lst_std = torch.from_numpy(np.array([11.376636496297289]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b37c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(path_to_data, train_perc, test_perc):\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path_to_data):\n",
    "        files.extend(filenames)\n",
    "    \n",
    "    saturated = files[:-1]\n",
    "    unsaturated = files[-1]\n",
    "    \n",
    "    dataset = wds.WebDataset(path_to_data + \"/\" + unsaturated)\n",
    "    counter = 0\n",
    "    for data in dataset:\n",
    "        counter += 1\n",
    "    \n",
    "    total_files = counter + len(saturated) * 10000\n",
    "    training_data = total_files * train_perc //10000\n",
    "    test_data_files = total_files * test_perc //10000\n",
    "\n",
    "    training_data = random.sample(files, int(training_data))\n",
    "    test_data = [file for file in files if file not in training_data]\n",
    "    test_data = random.sample(test_data, int(test_data_files))\n",
    "    # Get sample sizes of train and test data\n",
    "    training_samples = 0\n",
    "    testing_samples = 0\n",
    "    \n",
    "    for path in training_data:\n",
    "        if path in saturated:\n",
    "            training_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            training_samples += counter\n",
    "            \n",
    "    for path in test_data:\n",
    "        if path in saturated:\n",
    "            testing_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            testing_samples += counter\n",
    "            \n",
    "            \n",
    "    # Convert to filename lists \n",
    "    training_filepath = []\n",
    "    for dat in training_data:\n",
    "        training_filepath.append(dat[6:12])\n",
    "    training_path = path_to_data + \"shard-\" + \"{\" + \",\".join(training_filepath) + \"}\" + \".tar\"\n",
    "    \n",
    "    testing_filepath = []\n",
    "    for dat in test_data:\n",
    "        testing_filepath.append(dat[6:12])\n",
    "    testing_path = path_to_data + \"shard-{\" + \",\".join(testing_filepath) +\"}.tar\"\n",
    "    train_data = wds.WebDataset(training_path).shuffle(10000, initial=10000).decode(\"rgb\").rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "    test_data = wds.WebDataset(testing_path).decode(\"rgb\").shuffle(10000, initial=10000).rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "            \n",
    "    return (train_data, training_samples), (test_data, testing_samples)\n",
    "    \n",
    "(train_data, training_samples_len), (test_data, testing_samples_len) = create_train_test(PATH_TO_DATA, 0.85, 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a999e081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 33, 33)\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "for data in train_data:\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92291d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    This function will split our image into patches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, in_channels=3, embedding=768):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (int(image_size/patch_size))**2\n",
    "\n",
    "        self.patching_conv = nn.Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=embedding,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert single image to embedding x (root n x root n patches)\n",
    "        x = self.patching_conv(x)\n",
    "\n",
    "        # Flatten on second dimension to get embedding x n patches\n",
    "        x = x.flatten(2)\n",
    "\n",
    "        # Transpose to get n patches x embedding, thus giving each patch a 768 vector embedding\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Build the attention mechanism (nearly identical to original Transformer Paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, num_heads, qkv_b=True, attention_drop_p=0, projection_drop_p=0):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding # Size of embedding vector\n",
    "        self.num_heads = num_heads # Number of heads in multiheaded attention layers\n",
    "        self.qkv_b = qkv_b # Do we want a bias term on our QKV linear layer\n",
    "        self.attention_drop_p = attention_drop_p # Attention layer dropout probability\n",
    "        self.projection_drop_p = projection_drop_p # Projection layer dropout probability\n",
    "        self.head_dimension = int(self.embedding/self.num_heads) # Dimension of each head in multiheaded attention\n",
    "        self.scaling = self.head_dimension ** 0.5 # Scaling recommended by original transformer paper for exploding grad\n",
    "\n",
    "\n",
    "        self.qkv = nn.Linear(embedding, embedding * 3)\n",
    "        self.attention_drop = nn.Dropout(self.attention_drop_p)\n",
    "        self.projection = nn.Linear(embedding, embedding)\n",
    "        self.projection_drop = nn.Dropout(self.projection_drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get shape of input layer, samples x patches + patchembedding (1) x embedding\n",
    "        samples, patches, embedding = x.shape # (samples, patches+1, embedding)\n",
    "\n",
    "        # Expand embedding to 3 x embedding for QKV\n",
    "        qkv = self.qkv(x) # (sample, patches+1, 3*embedding)\n",
    "\n",
    "        # Reshape so that for every patch + 1 in every sample we have QKV with dimension number of heads by its dimension\n",
    "        # Remember that num_heads * head_dimension = embedding\n",
    "        qkv = qkv.reshape(samples, patches, 3, self.num_heads, self.head_dimension) # (samples, patches+1, 3, num_heads, head_dim)\n",
    "        \n",
    "        # Permute such that we have QKV so each has all samples, and each head in each sample\n",
    "        # has dimensions patches + 1 by heads dimension\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, samples, heads, patches+1, head_dim)\n",
    "\n",
    "        # Separate out QKV\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Transpose patches and head dimension of K\n",
    "        transpose_k = k.transpose(-2, -1) # (samples, heads, head_dim, patches+1)\n",
    "\n",
    "        # Matrix Multiplication of Q and K scaled\n",
    "        # (samples, heads, patches+1, head_dim) (samples, heads, head_dim, patches + 1)\n",
    "        # output: (sample, heads, patches+1, patches+1)\n",
    "        scaled_mult = torch.matmul(q, transpose_k) / self.scaling\n",
    "\n",
    "        # Run scaled multiplication through softmax layer along last dimension\n",
    "        attention = scaled_mult.softmax(dim=-1)\n",
    "        attention = self.attention_drop(attention)\n",
    "\n",
    "        # Calculate weighted average along V\n",
    "        # (sample, heads, patches+1, patches+1) x (samples, heads, patches+1, head_dim)\n",
    "        # Output (sample, heads, patches+1, head_dim)\n",
    "        weighted_average = torch.matmul(attention, v)\n",
    "\n",
    "        # Transpose to (samples, patches+1, heads, head_dim)\n",
    "        weighted_average = weighted_average.transpose(1,2)\n",
    "\n",
    "        # Flatten on last layer to get back original shape of (sample, patches + 1, embedding)\n",
    "        weighted_average = weighted_average.flatten(2)\n",
    "\n",
    "        # Run through our projection layer with dropout\n",
    "        x = self.projection_drop(self.projection(x))\n",
    "        return x\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Multi Layer Perceptron with GELU activation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, hidden_features, output_features, dropout_p=0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_features, hidden_features)\n",
    "        self.drop_1 = nn.Dropout(dropout_p)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, output_features)\n",
    "        self.drop_2 = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop_1(self.gelu(self.fc1(x)))\n",
    "        x = self.drop_2(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Create Self Attention Block with alyer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, num_heads, hidden_features=2048,qkv_b=True, attention_dropout_p=0,\n",
    "                 projection_dropout_p=0, mlp_dropout_p=0):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embedding, eps=1e-6)\n",
    "        self.attention = Attention(embedding=embedding,\n",
    "                                   num_heads=num_heads,\n",
    "                                   qkv_b=qkv_b,\n",
    "                                   attention_drop_p=attention_dropout_p,\n",
    "                                   projection_drop_p=projection_dropout_p)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding, eps=1e-6)\n",
    "        self.feedforward = MultiLayerPerceptron(input_features=embedding,\n",
    "                                                hidden_features=hidden_features,\n",
    "                                                output_features=embedding,\n",
    "                                                dropout_p=mlp_dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm1(x))\n",
    "        x = x + self.feedforward(self.layernorm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Putting together the Vision Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size=512, patch_size=16, in_channels=3, num_outputs=1000, embeddings=768,\n",
    "                 num_blocks=12, num_heads=12, hidden_features=2048, qkv_b=True, attention_dropout_p=0,\n",
    "                 projection_dropout_p=0, mlp_dropout_p=0, pos_embedding_dropout=0):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(image_size=image_size,\n",
    "                                              patch_size=patch_size,\n",
    "                                              in_channels=in_channels,\n",
    "                                              embedding=embeddings)\n",
    "        self.class_token = nn.Parameter(torch.zeros(size=(1,1,embeddings)))\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(size=(1,1+self.patch_embedding.num_patches, embeddings)))\n",
    "        self.positional_dropout = nn.Dropout(pos_embedding_dropout)\n",
    "        self.transformer_block = TransformerBlock(embedding=embeddings,\n",
    "                                                  num_heads=num_heads,\n",
    "                                                  hidden_features=hidden_features,\n",
    "                                                  qkv_b=qkv_b,\n",
    "                                                  attention_dropout_p=attention_dropout_p,\n",
    "                                                  projection_dropout_p=projection_dropout_p,\n",
    "                                                  mlp_dropout_p=mlp_dropout_p)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            self.transformer_block for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(embeddings, eps=1e-6)\n",
    "        self.out = nn.Linear(embeddings + 7, num_outputs)\n",
    "\n",
    "    def forward(self, x, forcing):\n",
    "        num_samples = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "        class_token = self.class_token.expand(num_samples, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        x = x + self.positional_embedding\n",
    "        x = self.positional_dropout(x)\n",
    "\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        output_class_token = x[:, 0]\n",
    "        output_class_token = torch.cat((output_class_token, forcing), dim=1)\n",
    "        x = self.out(output_class_token)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602dd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(image_size=33, patch_size=3, in_channels=5, num_outputs=1, embeddings=768,\n",
    "                        num_blocks=6, num_heads=12, hidden_features=1024, qkv_b=True, attention_dropout_p=0.2,\n",
    "                        projection_dropout_p=0.2, mlp_dropout_p=0.2, pos_embedding_dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbb461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** EPOCH: [0/50] LR: 0.0005 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                                          | 187/6838 [00:54<29:27,  3.76it/s, train_loss=8.35]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0005\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=6)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=6)\n",
    "\n",
    "model = vit.to(DEVICE)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0005)\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "lst_mean = lst_mean.to(DEVICE)\n",
    "lst_std = lst_std.to(DEVICE)\n",
    "forcing_mean = forcing_mean.to(DEVICE)\n",
    "forcing_std = forcing_std.to(DEVICE)\n",
    "\n",
    "def process_data(image, forcing, lst):\n",
    "    image, forcing, lst = image.to(DEVICE).to(torch.float32), forcing.to(DEVICE), lst.to(DEVICE)\n",
    "    image[:,4,] = torch.clip(image[:,4,], min=0, max=600)\n",
    "    image[:,:4,] = torch.clip(image[:,:4,], min=0, max=1)\n",
    "    image = image_normalize(image)\n",
    "    forcing = torch.div(torch.sub(forcing, forcing_mean), forcing_std).to(torch.float32)\n",
    "    # LST Transformation\n",
    "#     lst = torch.div(torch.sub(lst, lst_mean), lst_std).to(torch.float32).view(-1, 1)\n",
    "    lst = lst.view(-1, 1).to(torch.float32)\n",
    "    return image, forcing, lst\n",
    "\n",
    "\n",
    "min_test_loss = np.inf    \n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"****** EPOCH: [{}/{}] LR: {} ******\".format(epoch, EPOCHS, round(optimizer.param_groups[0]['lr'], 6)))\n",
    "    running_train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    running_test_loss = 0\n",
    "    test_n_iter = 0\n",
    "    \n",
    "    loop_train = tqdm(train_loader, total=(training_samples_len//BATCH_SIZE) + 1, leave=True)\n",
    "    for idx, (image, forcing, lst) in enumerate(loop_train):\n",
    "        image, forcing, lst = process_data(image, forcing, lst)\n",
    "        optimizer.zero_grad()\n",
    "        forward_out = model.forward(image, forcing)\n",
    "        loss = loss_fn(forward_out, lst)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "        loop_train.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    loop_test = tqdm(test_loader, total=(testing_samples_len//BATCH_SIZE) + 1, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (image, forcing, lst) in enumerate(loop_test):\n",
    "            image, forcing, lst = process_data(image, forcing, lst)\n",
    "            pred = model.forward(image, forcing)\n",
    "            testloss = loss_fn(pred, lst)\n",
    "            running_test_loss += testloss.item()\n",
    "            test_n_iter += 1\n",
    "            loop_test.set_postfix(test_loss=testloss.item())\n",
    "\n",
    "    avg_train_loss = running_train_loss/train_n_iter\n",
    "    train_loss.append(avg_train_loss)\n",
    "    avg_test_loss = running_test_loss/test_n_iter\n",
    "    test_loss.append(avg_test_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    scheduler2.step(avg_test_loss)\n",
    "    if avg_test_loss < min_test_loss:\n",
    "        print(\"Saving Model\")\n",
    "        min_test_loss = avg_test_loss\n",
    "        torch.save(model.state_dict(), \"resnetmodel.pt\")\n",
    "    print(\"------ Train Loss: {}, Test Loss: {} ------\".format(avg_train_loss, avg_test_loss))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb682bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot lines\n",
    "plt.plot(list(range(0,18)), train_loss, label = \"train_loss\")\n",
    "plt.plot(list(range(0,18)), test_loss, label = \"test_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Error LST Prediction\")\n",
    "plt.savefig(\"Training Curve.png\")\n",
    "plt.ylim([0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4871f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
