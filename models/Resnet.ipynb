{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fd6547-4abf-4b41-bac4-7e953a3bf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure about the cuda version\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms \n",
    "import os\n",
    "import random\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "PATH_TO_DATA = \"/glade/scratch/yiwenz/TransferLearningData/rand_shard_data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fccd78c-5260-49d3-9546-d10687b5c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_normalize = transforms.Normalize(\n",
    "                  mean=[0.17960437768666657, 0.14584139607643212, 0.10744440357398845, 0.2583671063835548],\n",
    "                  std=[0.059635202669355195, 0.04059554002618016, 0.03371736326989986, 0.06295501902505744]\n",
    ")\n",
    "\n",
    "forcing_normalize = transforms.Normalize(\n",
    "                  mean=[444.9605606256559, 991.7980623653417, 0.00039606951184754176, 96111.04161525163, 0.006652783216819315, 314.3219695851273, 2.82168247768119],\n",
    "                  std=[5.5216369223813535, 12.951212256256913, 0.0002824274832735609, 975.3770569179914, 0.00012386107613000674, 0.6004463118907452, 0.34279194598853185]\n",
    ")\n",
    "\n",
    "forcing_mean = torch.from_numpy(np.array([444.9605606256559, 991.7980623653417, 0.00039606951184754176, 96111.04161525163, 0.006652783216819315, 314.3219695851273, 2.82168247768119]))\n",
    "forcing_std = torch.from_numpy(np.array([5.5216369223813535, 12.951212256256913, 0.0002824274832735609, 975.3770569179914, 0.00012386107613000674, 0.6004463118907452, 0.34279194598853185]))\n",
    "\n",
    "lst_mean = torch.from_numpy(np.array([312.8291360088677]))\n",
    "lst_std = torch.from_numpy(np.array([11.376636496297289]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83520329-5aa7-47f6-b0c7-b50af7eee38e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000141\n",
      "000100\n",
      "000041\n",
      "000169\n",
      "000133\n",
      "000045\n",
      "000083\n",
      "000125\n",
      "000174\n",
      "000094\n",
      "000126\n",
      "000064\n",
      "000116\n",
      "000016\n",
      "000065\n",
      "000144\n",
      "000124\n",
      "000114\n",
      "000052\n",
      "000068\n",
      "000081\n",
      "000120\n",
      "000186\n",
      "000168\n",
      "000115\n",
      "000012\n",
      "000025\n",
      "000188\n",
      "000080\n",
      "000073\n",
      "000147\n",
      "000019\n",
      "000105\n",
      "000138\n",
      "000048\n",
      "000200\n",
      "000159\n",
      "000142\n",
      "000058\n",
      "000097\n",
      "000003\n",
      "000150\n",
      "000196\n",
      "000096\n",
      "000056\n",
      "000088\n",
      "000178\n",
      "000013\n",
      "000171\n",
      "000153\n",
      "000037\n",
      "000061\n",
      "000086\n",
      "000164\n",
      "000190\n",
      "000040\n",
      "000028\n",
      "000198\n",
      "000039\n",
      "000132\n",
      "000049\n",
      "000127\n",
      "000104\n",
      "000118\n",
      "000018\n",
      "000075\n",
      "000000\n",
      "000010\n",
      "000091\n",
      "000117\n",
      "000140\n",
      "000070\n",
      "000089\n",
      "000177\n",
      "000184\n",
      "000087\n",
      "000156\n",
      "000172\n",
      "000165\n",
      "000193\n",
      "000110\n",
      "000076\n",
      "000082\n",
      "000139\n",
      "000160\n",
      "000191\n",
      "000050\n",
      "000069\n",
      "000034\n",
      "000122\n",
      "000145\n",
      "000158\n",
      "000055\n",
      "000026\n",
      "000155\n",
      "000166\n",
      "000006\n",
      "000077\n",
      "000170\n",
      "000197\n",
      "000195\n",
      "000036\n",
      "000022\n",
      "000175\n",
      "000199\n",
      "000137\n",
      "000057\n",
      "000146\n",
      "000134\n",
      "000015\n",
      "000004\n",
      "000109\n",
      "000148\n",
      "000035\n",
      "000099\n",
      "000095\n",
      "000157\n",
      "000002\n",
      "000106\n",
      "000067\n",
      "000060\n",
      "000047\n",
      "000074\n",
      "000119\n",
      "000023\n",
      "000066\n",
      "000059\n",
      "000079\n",
      "000063\n",
      "000176\n",
      "000062\n",
      "000108\n",
      "000011\n",
      "000021\n",
      "000201\n",
      "000204\n",
      "000131\n",
      "000032\n",
      "000009\n",
      "000112\n",
      "000084\n",
      "000042\n",
      "000031\n",
      "000192\n",
      "000027\n",
      "000129\n",
      "000107\n",
      "000185\n",
      "000183\n",
      "000001\n",
      "000180\n",
      "000135\n",
      "000053\n",
      "000189\n",
      "000085\n",
      "000098\n",
      "000163\n",
      "000206\n",
      "000161\n",
      "000152\n",
      "000046\n",
      "000203\n",
      "000044\n",
      "000182\n",
      "000181\n",
      "000113\n",
      "000071\n",
      "000008\n",
      "000078\n",
      "000167\n",
      "000103\n",
      "000102\n",
      "000093\n",
      "000030\n",
      "000128\n",
      "000207\n"
     ]
    }
   ],
   "source": [
    "def create_train_test(path_to_data, train_perc, test_perc):\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path_to_data):\n",
    "        files.extend(filenames)\n",
    "    \n",
    "    saturated = files[:-1]\n",
    "    unsaturated = files[-1]\n",
    "    \n",
    "    dataset = wds.WebDataset(path_to_data + \"/\" + unsaturated)\n",
    "    counter = 0\n",
    "    for data in dataset:\n",
    "        counter += 1\n",
    "    \n",
    "    total_files = counter + len(saturated) * 10000\n",
    "    training_data = total_files * train_perc //10000\n",
    "    test_data_files = total_files * test_perc //10000\n",
    "\n",
    "    training_data = random.sample(files, int(training_data))\n",
    "    test_data = [file for file in files if file not in training_data]\n",
    "    test_data = random.sample(test_data, int(test_data_files))\n",
    "    # Get sample sizes of train and test data\n",
    "    training_samples = 0\n",
    "    testing_samples = 0\n",
    "    \n",
    "    for path in training_data:\n",
    "        if path in saturated:\n",
    "            training_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            training_samples += counter\n",
    "            \n",
    "    for path in test_data:\n",
    "        if path in saturated:\n",
    "            testing_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            testing_samples += counter\n",
    "            \n",
    "            \n",
    "    # Convert to filename lists \n",
    "    training_filepath = []\n",
    "    for dat in training_data:\n",
    "        print(dat[6:12])\n",
    "        training_filepath.append(dat[6:12])\n",
    "    training_path = path_to_data + \"shard-\" + \"{\" + \",\".join(training_filepath) + \"}\" + \".tar\"\n",
    "    \n",
    "    testing_filepath = []\n",
    "    for dat in test_data:\n",
    "        testing_filepath.append(dat[6:12])\n",
    "    testing_path = path_to_data + \"shard-{\" + \",\".join(testing_filepath) +\"}.tar\"\n",
    "    train_data = wds.WebDataset(training_path).shuffle(30000, initial=30000).decode(\"rgb\").rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "    test_data = wds.WebDataset(testing_path).decode(\"rgb\").shuffle(30000, initial=30000).rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "            \n",
    "    return (train_data, training_samples), (test_data, testing_samples)\n",
    "    \n",
    "(train_data, training_samples_len), (test_data, testing_samples_len) = create_train_test(PATH_TO_DATA, 0.85, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7645be-f539-4161-93aa-b2214e019914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "def process_data(image, forcing, lst):\n",
    "    image, forcing, lst = image.to(torch.float32).to(DEVICE), forcing.to(DEVICE), lst.to(DEVICE)\n",
    "    # Image Transformations\n",
    "    image = torch.clip(image, min=0, max=1)\n",
    "    image = image_normalize(image)\n",
    "    # Forcing Transformation\n",
    "    forcing = torch.div(torch.sub(forcing, forcing_mean), forcing_std).to(torch.float32)\n",
    "    # LST Transformation\n",
    "#     lst = torch.div(torch.sub(lst, lst_mean), lst_std).to(torch.float32).view(-1, 1)\n",
    "    lst = lst.view(-1, 1).to(torch.float32)\n",
    "    return image, forcing, lst\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers: List[int],\n",
    "        in_channel=4,\n",
    "        forcing_shape = 7,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.flatten_shape = None\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channel, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool1 = nn.AvgPool2d(2)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "#        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "#        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "#        self.fc0 = nn.Linear(512, out_features=64)\n",
    "        self.fc1 = nn.Linear(128+forcing_shape, out_features=1)\n",
    "#        self.fc2 = nn.Linear(in_features=2048, out_features=1)\n",
    "#        self.fc3 = nn.Linear(in_features=64, out_features=1)    \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block,\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor, forcing) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "#        x = self.conv0(x)\n",
    "#        x = x.permute([0,3,1,2])\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.avgpool1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "#        x = self.layer3(x)\n",
    "#        x = self.layer4(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "#        x = self.fc0(x)\n",
    "#        x = F.leaky_relu(x)\n",
    "        x = torch.cat((x, forcing), dim=1)\n",
    "        x = self.fc1(x)\n",
    "#        x = F.leaky_relu(x)\n",
    "#        x = self.fc2(x)\n",
    "#        x = F.leaky_relu(x)\n",
    "#        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet_simplified():\n",
    "    return ResNet(BasicBlock,[3,3,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e7ba9-6fc3-4466-a2ce-406b52d88f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# url_resnet18= \"https://download.pytorch.org/models/resnet18-5c106cde.pth\"\n",
    "# def load_pretrained_weights(model,url,mode):\n",
    "#     if mode == 'online':\n",
    "#         \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "#         state_dict = torch.utils.model_zoo.load_url(url)\n",
    "#         state_dict.pop(\"fc.weight\")\n",
    "#         state_dict.pop(\"fc.bias\")\n",
    "#         weight = state_dict['conv1.weight'].clone()\n",
    "#         state_dict.pop(\"conv1.weight\")\n",
    "#         model.load_state_dict(state_dict, strict=False)\n",
    "#         model.conv1.weight.data[:, :3] = weight\n",
    "#         model.conv1.weight.data[:, 3] = torch.mean(model.conv1.weight.data[:, :3],dim=1)\n",
    "#     elif mode == 'local':\n",
    "#         new_state_dict = OrderedDict()\n",
    "#         state_dict = torch.load('resnet_10.pt')\n",
    "#         for k, v in state_dict.items():\n",
    "#             name = k[7:] # remove `module.`\n",
    "#             new_state_dict[name] = v\n",
    "#         # load params\n",
    "#         model.load_state_dict(new_state_dict)\n",
    "# #    print(res.missing_keys)\n",
    "# #    assert set(res.missing_keys) == {\"fc.weight\", \"fc.bias\"}, \"issue loading pretrained weights\"\n",
    "#     print(f\"Loaded pretrained weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7587e52a-d137-423d-a290-00aeb4805348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=resnet_simplified()\n",
    "#load_pretrained_weights(model,url_resnet18,mode='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a529f836-ed9e-4c92-8a56-ca42b5b08662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Freeze imagenet parameters\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad and 'layer' in name:\n",
    "# #        print(name)\n",
    "#         param.requires_grad = False\n",
    "# model.bn1.weight.requires_grad = False\n",
    "# model.bn1.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557264e-0875-4e74-ab30-c1eaa4f75afd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1719 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** EPOCH: [0/100] LR: 0.0005 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:46, 16.17it/s, train_loss=5.73]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 97.12796076619348, Test Loss: 5.93673220872879 ------\n",
      "****** EPOCH: [1/100] LR: 0.000495 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:43, 16.67it/s, train_loss=5]                             \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 5.327743521679279, Test Loss: 4.969243290424347 ------\n",
      "****** EPOCH: [2/100] LR: 0.00049 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:43, 16.67it/s, train_loss=4.81]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.844923647614412, Test Loss: 4.704987785816193 ------\n",
      "****** EPOCH: [3/100] LR: 0.000485 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.39it/s, train_loss=4.7]                           \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.608911149446354, Test Loss: 4.499098998308182 ------\n",
      "****** EPOCH: [4/100] LR: 0.00048 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.43it/s, train_loss=4.69]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.460321586353834, Test Loss: 4.458783519268036 ------\n",
      "****** EPOCH: [5/100] LR: 0.000475 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:46, 16.19it/s, train_loss=4.5]                           \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.356376839515775, Test Loss: 4.372434899806977 ------\n",
      "****** EPOCH: [6/100] LR: 0.000471 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.46it/s, train_loss=4.37]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.282506649438725, Test Loss: 4.3074155926704405 ------\n",
      "****** EPOCH: [7/100] LR: 0.000466 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:56, 14.79it/s, train_loss=4.33]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.223782131283782, Test Loss: 4.239602183103561 ------\n",
      "****** EPOCH: [8/100] LR: 0.000461 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:45, 16.37it/s, train_loss=4.29]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.176427609837332, Test Loss: 4.21449528336525 ------\n",
      "****** EPOCH: [9/100] LR: 0.000457 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.53it/s, train_loss=4.18]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.134367743203806, Test Loss: 4.189360558986664 ------\n",
      "****** EPOCH: [10/100] LR: 0.000452 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:50, 15.51it/s, train_loss=4.21]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.1033315001532085, Test Loss: 4.184059134721756 ------\n",
      "****** EPOCH: [11/100] LR: 0.000448 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:49, 15.78it/s, train_loss=4.17]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.072650832353636, Test Loss: 4.136562192440033 ------\n",
      "****** EPOCH: [12/100] LR: 0.000443 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.44it/s, train_loss=4.14]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.048711363243502, Test Loss: 4.131272475719452 ------\n",
      "****** EPOCH: [13/100] LR: 0.000439 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.50it/s, train_loss=4.16]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.025700560558674, Test Loss: 4.087508246898651 ------\n",
      "****** EPOCH: [14/100] LR: 0.000434 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.47it/s, train_loss=4.05]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 4.004413953492808, Test Loss: 4.10721269607544 ------\n",
      "****** EPOCH: [15/100] LR: 0.00043 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:44, 16.42it/s, train_loss=4.12]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 3.9861567095268606, Test Loss: 4.1036288189888 ------\n",
      "****** EPOCH: [16/100] LR: 0.000426 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [01:45, 16.37it/s, train_loss=4.09]                          \n",
      "  0%|          | 0/1719 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train Loss: 3.9721432335154954, Test Loss: 4.066394786834717 ------\n",
      "****** EPOCH: [17/100] LR: 0.000421 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 1111/1719 [01:11<00:34, 17.70it/s, train_loss=3.86]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "DECAY_RATE = 0.99\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model = torch.nn.DataParallel(model)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.95)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=DECAY_RATE)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "lst_mean = lst_mean.to(DEVICE)\n",
    "lst_std = lst_std.to(DEVICE)\n",
    "forcing_mean = forcing_mean.to(DEVICE)\n",
    "forcing_std = forcing_std.to(DEVICE)\n",
    "\n",
    "min_test_loss = np.inf \n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"****** EPOCH: [{}/{}] LR: {} ******\".format(epoch, EPOCHS, round(optimizer.param_groups[0]['lr'], 6)))\n",
    "    running_train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    running_test_loss = 0\n",
    "    test_n_iter = 0\n",
    "    \n",
    "    # model.train()\n",
    "    loop_train = tqdm(train_loader, total=(training_samples_len//BATCH_SIZE) + 1, leave=True)\n",
    "    for idx, (image, forcing, lst) in enumerate(loop_train):\n",
    "        image, forcing, lst = process_data(image, forcing, lst)\n",
    "        optimizer.zero_grad()\n",
    "        forward_out = model.forward(image, forcing)\n",
    "        loss = loss_fn(forward_out, lst)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "        loop_train.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    loop_test = tqdm(test_loader, total=(testing_samples_len//BATCH_SIZE) + 1, leave=False)\n",
    "    \n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, forcing, lst) in enumerate(loop_test):\n",
    "            image, forcing, lst = process_data(image, forcing, lst)\n",
    "            pred = model.forward(image, forcing)\n",
    "            testloss = loss_fn(pred, lst)\n",
    "            running_test_loss += testloss.item()\n",
    "            test_n_iter += 1\n",
    "            loop_test.set_postfix(test_loss=testloss.item())\n",
    "\n",
    "    avg_train_loss = running_train_loss/train_n_iter\n",
    "    train_loss.append(avg_train_loss)\n",
    "    avg_test_loss = running_test_loss/test_n_iter\n",
    "    test_loss.append(avg_test_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    scheduler2.step(avg_test_loss)\n",
    "#     if avg_test_loss < min_test_loss:\n",
    "#         print(\"Saving Model\")\n",
    "#         min_test_loss = avg_test_loss\n",
    "#         torch.save(model.state_dict(), \"resnet_11.pt\")\n",
    "    \n",
    "#     print(\"Saving Loss\")\n",
    "#     file_name = \"train_loss_11.pkl\"\n",
    "#     open_file = open(file_name, \"wb\")\n",
    "#     pickle.dump(train_loss, open_file)\n",
    "#     open_file.close()\n",
    "#     file_name = \"test_loss_11.pkl\"\n",
    "#     open_file = open(file_name, \"wb\")\n",
    "#     pickle.dump(test_loss, open_file)\n",
    "#     open_file.close()\n",
    "    print(\"------ Train Loss: {}, Test Loss: {} ------\".format(avg_train_loss, avg_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d95398-5efd-4cac-8232-43772a3b35d6",
   "metadata": {},
   "source": [
    "# Igonre stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741573ab-edee-4dc7-b552-ae4077af7f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"train_loss_11.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "train_loaded = pickle.load(open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_11.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "test_loaded = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58cb713e-dc99-44b7-8e59-27306fd65279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(train_loaded, label=\"train_loss\")\n",
    "line2, = ax.plot(test_loaded, label=\"test_loss\")\n",
    "ax.set_ylim([0, 10])\n",
    "# Create a legend for the first line.\n",
    "first_legend = ax.legend(loc='upper right')\n",
    "# Add the legend manually to the Axes.\n",
    "ax.add_artist(first_legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f2c975-5cf3-41e8-92ec-45b3a24347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name = \"train_loss.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(train_loss, open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = \"test_loss.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(test_loss, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacde853-3432-4377-b540-0e48c5f6e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_4=train_loaded+train_loss\n",
    "test_4=test_loaded+test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ac7a28-156c-49e0-8f46-862a8fecede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"train_loss_4.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(train_4, open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_4.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(test_4, open_file)\n",
    "open_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL3 (my_npl_clone)",
   "language": "python",
   "name": "npl3-my_npl_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
