{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fd6547-4abf-4b41-bac4-7e953a3bf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure about the cuda version\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms \n",
    "import os\n",
    "import random\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "PATH_TO_DATA = \"/glade/scratch/yiwenz/TransferLearningData/rand_sharded_data_all_Daily/\" \n",
    "#PATH_TO_DATA = \"/glade/scratch/yiwenz/TransferLearningData/rand_shard_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fccd78c-5260-49d3-9546-d10687b5c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 channels: ['Red', 'Green', 'Blue', \"NIR\", \"SWIR1\",\"ndbi\",\"ndvi\",\"elevation\"]\n",
    "\n",
    "image_normalize = transforms.Normalize(\n",
    "                  mean=[0, 0, 0, 0, 0, 0, 0, 4.1459e+02],\n",
    "                  std=[1, 1, 1, 1, 1, 1, 1, 8.9265e+01]\n",
    ")\n",
    "image_rotate = transforms.RandomRotation(90)\n",
    "#image mean(no clip of elev): [1.8218e-01,  1.4804e-01,  1.0894e-01,  2.6186e-01,  2.5452e-01, -9.3340e-03,  9.4021e-02,  4.1459e+02]\n",
    "#image std (no clip of elev): [5.6672e-02, 4.0446e-02, 3.3400e-02, 6.2702e-02, 7.1029e-02, 5.7446e-02, 7.2435e-02, 8.9265e+01]\n",
    "\n",
    "# forcing_normalize = transforms.Normalize(\n",
    "#                   mean=[3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02, 2.7644e+00],\n",
    "#                   std=[6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00, 1.5957e+00]\n",
    "# )\n",
    "\n",
    "forcing_mean = torch.from_numpy(np.array([3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02, 2.7644e+00]))\n",
    "forcing_std = torch.from_numpy(np.array([6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00, 1.5957e+00]))\n",
    "# forcing mean:[3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02, 2.7644e+00]\n",
    "# forcing std: [6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00, 1.5957e+00]\n",
    "\n",
    "lst_mean = torch.from_numpy(np.array([315.1010]))\n",
    "lst_std = torch.from_numpy(np.array([10.9206]))\n",
    "#lst mean:315.1010\n",
    "#lst std: 10.9206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83520329-5aa7-47f6-b0c7-b50af7eee38e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/scratch/yiwenz/TransferLearningData/rand_sharded_data_all_Daily/shard-{000230,000223,000464,000711,000186,000894,001127,000132,001173,001171,000058,000154,001014,000655,000086,000271,000373,000866,001183,001072,000438,000999,000940,000031,000494,001151,000387,000317,000308,000278,000646,000236,000248,000667,000671,000404,000564,000575,000496,000393,000410,001041,000288,000596,000855,001221,000224,000521,000589,000798,000267,001043,000874,001196,000875,000234,000649,000713,000254,000126,000872,001011,000213,000889,001044,000064,000840,000246,000111,000415,001122,000277,000966,000493,000309,000513,000421,000715,000311,000631,000572,000517,000992,001026,000322,000461,000714,000956,000321,000582,000079,001108,000545,000910,000073,000282,000704,000663,000678,000054,001109,000601,000272,001051,000607,000423,000936,000518,000462,000851,000924,000024,000645,000603,000538,000369,000338,000255,000557,001048,000654,000185,000418,000612,000954,000653,000480,000979,000001,000364,000368,000552,000570,000523,000511,001197,001004,000590,000182,000110,000366,000580,000839,000250,000181,000831,000793,000124,000633,000401,001147,000944,000499,000128,001166,001205,000594,000937,000244,000218,000508,000726,000736,000856,000117,000092,001097,000628,000269,000697,001190,000341,000296,000306,001187,000534,000750,001058,000119,000816,001008,000656,000712,000929,000971,000794,001129,001093,000026,000558,000038,000101,000409,000165,000525,000721,000679,001034,000725,000842,000090,000566,000706,000344,000023,001042,000871,000837,000950,000113,000353,000970,001192,000892,000235,001040,001195,000988,000029,000009,000893,000727,000827,000005,000046,000780,000526,000776,000700,000159,000811,000920,000808,000412,000698,000907,000490,001082,000022,000796,001113,000118,000527,000160,000056,000914,000130,000643,000004,000429,000858,000314,000899,001220,000890,000424,000657,000226,001070,000088,000087,000802,000846,000707,000767,000852,000329,000705,000203,000003,000647,000006,000791,000516,000738,000485,000482,000442,000283,000179,000748,000501,000532,000548,000695,000426,000358,000728,000240,000625,000878,001214,000917,000984,000072,001052,000926,000862,001189,001206,000313,000571,000231,001046,000303,000760,000444,000078,000247,000114,000528,001076,000666,000987,000629,001216,000434,000932,001005,001164,000948,000778,000263,000786,000809,000873,001066,000400,001015,000743,001079,000976,000099,000112,001110,000991,000013,000274,000172,000947,001086,000915,000969,001061,000177,001112,001106,000080,001186,000960,000799,001094,000202,000137,001028,000451,000016,000249,000694,000702,000585,001161,000676,001198,001103,000777,000189,000972,000577,000854,001154,000084,001156,000673,000544,000437,000048,000161,000662,001017,000547,000635,001174,000145,000941,001131,000568,001049,000955,000397,000696,000285,000833,000243,000405,001200,000672,001172,000034,000638,000021,000100,000624,000298,000431,000995,001178,000503,000089,000342,000040,000039,000388,000965,000506,000502,000486,001115,000998,000477,000116,000930,001069,000043,000324,000036,000824,000898,000749,000302,000459,000294,000273,000542,000931,000396,000389,000327,000690,000788,000330,000199,001145,000192,000729,000636,000174,000233,000844,000630,000814,000062,000533,000276,001184,000578,000136,000191,000761,000371,000328,000195,000946,000642,000133,000519,000466,000053,000888,000295,000870,000792,000555,000815,001199,001138,001143,001204,000279,000848,000919,000975,000027,000055,000408,000020,000887,000765,000000,001139,000830,000215,001074,000008,000012,000906,000094,000683,000859,000560,000817,001077,001179,000432,000355,000428,000722,000500,000131,000340,000718,001002,001027,000608,000107,001194,000933,000281,000413,000018,000530,000484,000287,000265,001021,001018,000869,000200,000622,000838,000305,000812,000219,000348,000551,000771,000549,000880,000509,000579,000377,000510,001176,000472,001142,000205,000417,000153,000121,000245,000927,000440,000801,000175,000059,000836,001152,000598,000319,000134,001062,000194,000916,000208,000756,000685,000188,000922,001213,000357,000699,001168,001169,000163,000362,000384,000211,000677,001155,000458,001033,000214,001107,000268,001080,000576,000595,000674,000515,000863,000977,001201,000512,000035,000990,000435,000843,000105,000804,001193,000619,000968,000220,000800,000783,000973,000275,000505,000912,000709,000354}.tar\n"
     ]
    }
   ],
   "source": [
    "def create_train_test(path_to_data, train_perc, test_perc):\n",
    "    random.seed(42)\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path_to_data):\n",
    "        files.extend(filenames)\n",
    "    \n",
    "    saturated = files[:-1] #-1\n",
    "    unsaturated = files[-1] #-1\n",
    "    \n",
    "    dataset = wds.WebDataset(path_to_data + \"/\" + unsaturated)\n",
    "    counter = 0\n",
    "    for data in dataset:\n",
    "        counter += 1\n",
    "    \n",
    "    total_files = counter + len(saturated) * 10000\n",
    "    training_data = total_files * train_perc //10000\n",
    "    test_data_files = total_files * test_perc //10000\n",
    "\n",
    "    training_data = random.sample(files, int(training_data))\n",
    "    test_data = [file for file in files if file not in training_data]\n",
    "    test_data = random.sample(test_data, int(test_data_files))\n",
    "    # Get sample sizes of train and test data\n",
    "    training_samples = 0\n",
    "    testing_samples = 0\n",
    "    \n",
    "    for path in training_data:\n",
    "        if path in saturated:\n",
    "            training_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            training_samples += counter\n",
    "            \n",
    "    for path in test_data:\n",
    "        if path in saturated:\n",
    "            testing_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            testing_samples += counter\n",
    "            \n",
    "            \n",
    "    # Convert to filename lists \n",
    "    training_filepath = []\n",
    "    for dat in training_data:\n",
    "#        print(dat[6:12])\n",
    "        training_filepath.append(dat[6:12])\n",
    "    training_path = path_to_data + \"shard-\" + \"{\" + \",\".join(training_filepath) + \"}\" + \".tar\"\n",
    "    print(training_path)\n",
    "    \n",
    "    testing_filepath = []\n",
    "    for dat in test_data:\n",
    "        testing_filepath.append(dat[6:12])\n",
    "    testing_path = path_to_data + \"shard-{\" + \",\".join(testing_filepath) +\"}.tar\"\n",
    "    train_data = wds.WebDataset(training_path).shuffle(30000, initial=30000).decode(\"rgb\").rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "    test_data = wds.WebDataset(testing_path).decode(\"rgb\").shuffle(30000, initial=30000).rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "#    print(testing_path)\n",
    "    \n",
    "    all_filepath = []\n",
    "    for dat in saturated:\n",
    "#        print(dat[6:12])\n",
    "        all_filepath.append(dat[6:12])\n",
    "    all_path = path_to_data + \"shard-\" + \"{\" + \",\".join(all_filepath) + \"}\" + \".tar\"\n",
    "#    print(all_path)\n",
    "    all_data = wds.WebDataset(all_path).decode(\"rgb\").rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\").to_tuple(\"image\", \"forcing\", \"lst\")\n",
    "   \n",
    "    \n",
    "    return (train_data, training_samples), (test_data, testing_samples),all_data\n",
    "    \n",
    "(train_data, training_samples_len), (test_data, testing_samples_len), all_data = create_train_test(PATH_TO_DATA, 0.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e8d1bd-43b7-422d-b3fc-59504490d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataloader):\n",
    "    img_channels_sum, img_channels_squared_sum, forcing_channels_sum, forcing_channels_squared_sum, lst_channels_sum, lst_channels_squared_sum, num_batches = 0, 0, 0, 0, 0, 0, 0\n",
    "    for img, forcing, lst in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        img[:,:5,] = torch.clip(img[:,:5,], min=0, max=1)\n",
    "        image[:,5:7,] = torch.clip(image[:,5:7,], min=-1, max=1)\n",
    "#        img[:,-1,] = torch.clip(img[:,-1,], min=0, max=600)\n",
    "        \n",
    "        img_channels_sum += torch.mean(img, dim=[0,2,3])\n",
    "        img_channels_squared_sum += torch.mean(img**2, dim=[0,2,3])\n",
    "        \n",
    "        forcing_channels_sum += torch.mean(forcing, dim=[0])\n",
    "        forcing_channels_squared_sum += torch.mean(forcing**2, dim=[0])\n",
    "        lst_channels_sum += torch.mean(lst, dim=[0])\n",
    "        lst_channels_squared_sum += torch.mean(lst**2, dim=[0])\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    img_mean = img_channels_sum / num_batches\n",
    "    forcing_mean = forcing_channels_sum / num_batches\n",
    "    lst_mean = lst_channels_sum / num_batches\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    img_std = (img_channels_squared_sum / num_batches - img_mean ** 2) ** 0.5\n",
    "    forcing_std = (forcing_channels_squared_sum / num_batches - forcing_mean ** 2) ** 0.5\n",
    "    lst_std = (lst_channels_squared_sum / num_batches - lst_mean ** 2) ** 0.5\n",
    "    \n",
    "\n",
    "    return img_mean, img_std, forcing_mean, forcing_std, lst_mean, lst_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c948c60-1230-431d-a75d-1c3664f2a0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.8218e-01,  1.4804e-01,  1.0894e-01,  2.6186e-01,  2.5452e-01,\n",
       "         -9.3340e-03,  9.4021e-02,  4.1459e+02], dtype=torch.float64),\n",
       " tensor([5.6672e-02, 4.0446e-02, 3.3400e-02, 6.2702e-02, 7.1029e-02, 5.7446e-02,\n",
       "         7.2435e-02, 8.9265e+01], dtype=torch.float64),\n",
       " tensor([3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02,\n",
       "         2.7644e+00]),\n",
       " tensor([6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00,\n",
       "         1.5957e+00]),\n",
       " tensor(315.1010, dtype=torch.float64),\n",
       " tensor(10.9206, dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(all_data, batch_size=1024, num_workers=8)\n",
    "get_mean_and_std(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7e02a0-7923-4786-9ac4-ca2caf1a1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image, forcing, lst):\n",
    "    image, forcing, lst = image.to(DEVICE).to(torch.float32), forcing.to(DEVICE), lst.to(DEVICE)\n",
    "    # Image Transformations\n",
    "    image[:,7,] = torch.clip(image[:,7,], min=0, max=600)\n",
    "    image[:,:5,] = torch.clip(image[:,:5,], min=0, max=1)\n",
    "    image[:,5:7,] = torch.clip(image[:,5:7,], min=-1, max=1)\n",
    "    image = image_normalize(image)\n",
    "    image = image[:,:7,:] #np.array([5,6])\n",
    "#    image = image_rotate(image)\n",
    "    # Forcing Transformation\n",
    "    forcing = torch.div(torch.sub(forcing, forcing_mean), forcing_std).to(torch.float32)\n",
    "#    forcing = forcing.unsqueeze(2).unsqueeze(3)\n",
    "#    forcing = forcing.repeat(1,1,33,33)\n",
    "    # LST Transformation\n",
    "#     lst = torch.div(torch.sub(lst, lst_mean), lst_std).to(torch.float32).view(-1, 1)\n",
    "    lst = lst.view(-1, 1).to(torch.float32)\n",
    "    return image, forcing, lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e3cf51-a6c0-4c63-ab48-5ebe241e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "#        out = F.dropout2d(out, p=0.3)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "#        out = F.dropout2d(out, p=0.1)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class SE_Block(nn.Module):\n",
    "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, r=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        # add SE block\n",
    "        self.se = SE_Block(planes, r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # add SE operation\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7645be-f539-4161-93aa-b2214e019914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers: List[int],\n",
    "        in_channel=7,\n",
    "        forcing_shape = 7,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.flatten_shape = None\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channel, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool1 = nn.AvgPool2d(2)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "#        self.fc0 = nn.Linear(512, out_features=64)\n",
    "        self.fc1 = nn.Linear(512+forcing_shape, out_features=1024)\n",
    "        self.drop1 = nn.Dropout(0.02)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block,\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor, forcing) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "#        x = self.conv0(x)\n",
    "#        x = x.permute([0,3,1,2])\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.cat((x, forcing), dim=1)\n",
    "#        x = self.fc0(x)\n",
    "#        x = F.leaky_relu(x)        \n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "#        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "#        x = F.leaky_relu(x)\n",
    "#        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet_simplified():\n",
    "    return ResNet(BasicBlock,[3,3,3,3])\n",
    "def resnet_bottleneck():\n",
    "    return ResNet(Bottleneck,[3,3,0,0])\n",
    "def se_resnet():\n",
    "    return ResNet(SEBasicBlock,[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0e7ba9-6fc3-4466-a2ce-406b52d88f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# url_resnet18= \"https://download.pytorch.org/models/resnet18-5c106cde.pth\"\n",
    "# def load_pretrained_weights(model,url,mode):\n",
    "#     if mode == 'online':\n",
    "#         \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "#         state_dict = torch.utils.model_zoo.load_url(url)\n",
    "#         state_dict.pop(\"fc.weight\")\n",
    "#         state_dict.pop(\"fc.bias\")\n",
    "#         weight = state_dict['conv1.weight'].clone()\n",
    "#         state_dict.pop(\"conv1.weight\")\n",
    "#         model.load_state_dict(state_dict, strict=False)\n",
    "#         model.conv1.weight.data[:, :3] = weight\n",
    "#         model.conv1.weight.data[:, 3] = torch.mean(model.conv1.weight.data[:, :3],dim=1)\n",
    "#     elif mode == 'local':\n",
    "#         new_state_dict = OrderedDict()\n",
    "#         state_dict = torch.load('resnet_10.pt')\n",
    "#         for k, v in state_dict.items():\n",
    "#             name = k[7:] # remove `module.`\n",
    "#             new_state_dict[name] = v\n",
    "#         # load params\n",
    "#         model.load_state_dict(new_state_dict)\n",
    "# #    print(res.missing_keys)\n",
    "# #    assert set(res.missing_keys) == {\"fc.weight\", \"fc.bias\"}, \"issue loading pretrained weights\"\n",
    "#     print(f\"Loaded pretrained weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7587e52a-d137-423d-a290-00aeb4805348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=resnet_simplified()\n",
    "#load_pretrained_weights(model,url_resnet18,mode='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a529f836-ed9e-4c92-8a56-ca42b5b08662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Freeze imagenet parameters\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad and 'layer' in name:\n",
    "# #        print(name)\n",
    "#         param.requires_grad = False\n",
    "# model.bn1.weight.requires_grad = False\n",
    "# model.bn1.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557264e-0875-4e74-ab30-c1eaa4f75afd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5967 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** EPOCH: [0/100] LR: 0.001 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [11:01<00:00,  9.01it/s, train_loss=2.31]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5967 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 3.51137351026156, Test Loss: 1.8525665224993906 ------\n",
      "****** EPOCH: [1/100] LR: 0.00096 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:57<00:00,  9.06it/s, train_loss=1.64]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 2.1312064548156893, Test Loss: 1.8735476617629712 ------\n",
      "****** EPOCH: [2/100] LR: 0.000922 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:55<00:00,  9.10it/s, train_loss=1.62]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5967 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.9651444454871227, Test Loss: 1.7557024162349892 ------\n",
      "****** EPOCH: [3/100] LR: 0.000885 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:58<00:00,  9.06it/s, train_loss=1.63]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.8847141212491847, Test Loss: 1.7287676077223941 ------\n",
      "****** EPOCH: [4/100] LR: 0.000849 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:46<00:00,  9.23it/s, train_loss=1.61]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.8384146495251654, Test Loss: 1.7154232180836209 ------\n",
      "****** EPOCH: [5/100] LR: 0.000815 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:46<00:00,  9.22it/s, train_loss=1.61]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.7942963969921397, Test Loss: 1.6500726618296326 ------\n",
      "****** EPOCH: [6/100] LR: 0.000783 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [11:34<00:00,  8.59it/s, train_loss=1.51]  \n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.7628391192460369, Test Loss: 1.6587504645853137 ------\n",
      "****** EPOCH: [7/100] LR: 0.000751 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:43<00:00,  9.26it/s, train_loss=1.57]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.7237922622516575, Test Loss: 1.6881488498836057 ------\n",
      "****** EPOCH: [8/100] LR: 0.000721 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:52<00:00,  9.14it/s, train_loss=1.67] \n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.691802075579725, Test Loss: 1.6076626393906648 ------\n",
      "****** EPOCH: [9/100] LR: 0.000693 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:50<00:00,  9.17it/s, train_loss=1.37]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.6656165170365684, Test Loss: 1.6390850323697794 ------\n",
      "****** EPOCH: [10/100] LR: 0.000665 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:45<00:00,  9.24it/s, train_loss=1.35]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.6399882408110424, Test Loss: 1.6035006249628736 ------\n",
      "****** EPOCH: [11/100] LR: 0.000638 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:53<00:00,  9.13it/s, train_loss=1.41]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.6227768161803429, Test Loss: 1.609815497661514 ------\n",
      "****** EPOCH: [12/100] LR: 0.000613 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [10:54<00:00,  9.11it/s, train_loss=1.34]\n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.605601305872047, Test Loss: 1.6097831168900365 ------\n",
      "****** EPOCH: [13/100] LR: 0.000588 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [11:01<00:00,  9.02it/s, train_loss=1.35]  \n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.5906915331014786, Test Loss: 1.5752598974218337 ------\n",
      "****** EPOCH: [14/100] LR: 0.000565 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [11:11<00:00,  8.89it/s, train_loss=1.27]  \n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.5774763794567448, Test Loss: 1.578886165168373 ------\n",
      "****** EPOCH: [15/100] LR: 0.000542 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [11:03<00:00,  8.99it/s, train_loss=1.3]  \n",
      "  0%|          | 0/5967 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.5704172168344974, Test Loss: 1.5638439777902138 ------\n",
      "****** EPOCH: [16/100] LR: 0.00052 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5963/5967 [11:04<00:00,  8.97it/s, train_loss=1.37] \n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5967 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.5559442462626067, Test Loss: 1.5632111272684308 ------\n",
      "****** EPOCH: [17/100] LR: 0.0005 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 3210/5967 [05:53<05:00,  9.17it/s, train_loss=1.19]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "DECAY_RATE = 0.96\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model = torch.nn.DataParallel(model)\n",
    "loss_fn = nn.SmoothL1Loss() #\n",
    "#loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.95)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=DECAY_RATE)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=5)\n",
    "\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "lst_mean = lst_mean.to(DEVICE)\n",
    "lst_std = lst_std.to(DEVICE)\n",
    "forcing_mean = forcing_mean.to(DEVICE)\n",
    "forcing_std = forcing_std.to(DEVICE)\n",
    "\n",
    "min_test_loss = np.inf \n",
    "torch.backends.cudnn.benchmark = True\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"****** EPOCH: [{}/{}] LR: {} ******\".format(epoch, EPOCHS, round(optimizer.param_groups[0]['lr'], 6)))\n",
    "    running_train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    running_test_loss = 0\n",
    "    test_n_iter = 0\n",
    "    \n",
    "    # model.train()\n",
    "    loop_train = tqdm(train_loader, total=(training_samples_len//BATCH_SIZE) + 1, leave=True)\n",
    "    for idx, (image, forcing, lst) in enumerate(loop_train):\n",
    "        image, forcing, lst = process_data(image, forcing, lst)\n",
    "        optimizer.zero_grad()\n",
    "        forward_out = model.forward(image, forcing)\n",
    "        loss = loss_fn(forward_out, lst)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "        loop_train.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    loop_test = tqdm(test_loader, total=(testing_samples_len//BATCH_SIZE) + 1, leave=False)\n",
    "    \n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, forcing, lst) in enumerate(loop_test):\n",
    "            image, forcing, lst = process_data(image, forcing, lst)\n",
    "            pred = model.forward(image, forcing)\n",
    "            testloss = loss_fn(pred, lst)\n",
    "            running_test_loss += testloss.item()\n",
    "            test_n_iter += 1\n",
    "            loop_test.set_postfix(test_loss=testloss.item())\n",
    "\n",
    "    avg_train_loss = running_train_loss/train_n_iter\n",
    "    train_loss.append(avg_train_loss)\n",
    "    avg_test_loss = running_test_loss/test_n_iter\n",
    "    test_loss.append(avg_test_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    scheduler2.step(avg_test_loss)\n",
    "    if avg_test_loss < min_test_loss:\n",
    "        print(\"Saving Model\")\n",
    "        min_test_loss = avg_test_loss\n",
    "        torch.save(model.state_dict(), \"resnet_new_2.pt\")\n",
    "    \n",
    "    print(\"Saving Loss\")\n",
    "    file_name = \"train_loss_new_2.pkl\"\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(train_loss, open_file)\n",
    "    open_file.close()\n",
    "    file_name = \"test_loss_new_2.pkl\"\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(test_loss, open_file)\n",
    "    open_file.close()\n",
    "    print(\"------ Train Loss: {}, Test Loss: {} ------\".format(avg_train_loss, avg_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d95398-5efd-4cac-8232-43772a3b35d6",
   "metadata": {},
   "source": [
    "# Igonre stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741573ab-edee-4dc7-b552-ae4077af7f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"train_loss_new_1.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "train_loaded = pickle.load(open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_new_1.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "test_loaded = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cb713e-dc99-44b7-8e59-27306fd65279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(train_loaded, label=\"train_loss\")\n",
    "line2, = ax.plot(test_loaded, label=\"test_loss\")\n",
    "ax.set_ylim([0, 3])\n",
    "# Create a legend for the first line.\n",
    "first_legend = ax.legend(loc='upper right')\n",
    "# Add the legend manually to the Axes.\n",
    "ax.add_artist(first_legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f2c975-5cf3-41e8-92ec-45b3a24347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name = \"train_loss.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(train_loss, open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = \"test_loss.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(test_loss, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacde853-3432-4377-b540-0e48c5f6e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_4=train_loaded+train_loss\n",
    "test_4=test_loaded+test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ac7a28-156c-49e0-8f46-862a8fecede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"train_loss_4.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(train_4, open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_4.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(test_4, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5d5b1ed-5e32-44fb-a245-0e5832555663",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7],[8,9,10,11,12,13,14]])\n",
    "x = x.unsqueeze(2).unsqueeze(3)\n",
    "x = x.repeat(1,1,3,3)\n",
    "y = x\n",
    "z = torch.cat((x,y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "991123d9-6950-4c0c-909a-f9613c865acd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1]],\n",
       "\n",
       "         [[ 2]],\n",
       "\n",
       "         [[ 3]],\n",
       "\n",
       "         [[ 4]],\n",
       "\n",
       "         [[ 5]],\n",
       "\n",
       "         [[ 6]],\n",
       "\n",
       "         [[ 7]]],\n",
       "\n",
       "\n",
       "        [[[ 8]],\n",
       "\n",
       "         [[ 9]],\n",
       "\n",
       "         [[10]],\n",
       "\n",
       "         [[11]],\n",
       "\n",
       "         [[12]],\n",
       "\n",
       "         [[13]],\n",
       "\n",
       "         [[14]]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1],[2],[3],[4],[5],[6],[7]],[[8],[9],[10],[11],[12],[13],[14]]])\n",
    "x = torch.unsqueeze(x, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcee774-091a-4b94-9ebd-b02f29fa8712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 3, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.zeros((3,3,3,3))\n",
    "x[:,np.array([0,2]),:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL3 (my_npl_clone)",
   "language": "python",
   "name": "npl3-my_npl_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
