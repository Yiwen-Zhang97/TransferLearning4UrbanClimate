{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd6547-4abf-4b41-bac4-7e953a3bf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure about the cuda version\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms \n",
    "import os\n",
    "import random\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "PATH_TO_DATA = \"/glade/scratch/yiwenz/TransferLearningData/rand_sharded_data_all_Daily_dropna/\" \n",
    "#PATH_TO_DATA = \"/glade/scratch/yiwenz/TransferLearningData/rand_shard_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fccd78c-5260-49d3-9546-d10687b5c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 channels: ['Red', 'Green', 'Blue', \"NIR\", \"SWIR1\",\"ndbi\",\"ndvi\",\"elevation\"]\n",
    "\n",
    "image_normalize = transforms.Normalize(\n",
    "                  mean=[0, 0, 0, 0, 0, 0, 0, 4.1459e+02],\n",
    "                  std=[1, 1, 1, 1, 1, 1, 1, 8.9265e+01]\n",
    ")\n",
    "image_rotate = transforms.RandomRotation(90)\n",
    "#image mean(no clip of elev): [1.8218e-01,  1.4804e-01,  1.0894e-01,  2.6186e-01,  2.5452e-01, -9.3340e-03,  9.4021e-02,  4.1459e+02]\n",
    "#image std (no clip of elev): [5.6672e-02, 4.0446e-02, 3.3400e-02, 6.2702e-02, 7.1029e-02, 5.7446e-02, 7.2435e-02, 8.9265e+01]\n",
    "\n",
    "# forcing_normalize = transforms.Normalize(\n",
    "#                   mean=[3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02, 2.7644e+00],\n",
    "#                   std=[6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00, 1.5957e+00]\n",
    "# )\n",
    "\n",
    "forcing_mean = torch.from_numpy(np.array([3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02, 2.7644e+00]))\n",
    "forcing_std = torch.from_numpy(np.array([6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00, 1.5957e+00]))\n",
    "# forcing mean:[3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02, 2.7644e+00]\n",
    "# forcing std: [6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00, 1.5957e+00]\n",
    "\n",
    "lst_mean = torch.from_numpy(np.array([315.1010]))\n",
    "lst_std = torch.from_numpy(np.array([10.9206]))\n",
    "#lst mean:315.1010\n",
    "#lst std: 10.9206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83520329-5aa7-47f6-b0c7-b50af7eee38e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/scratch/yiwenz/TransferLearningData/rand_sharded_data_all_Daily_dropna/shard-{000721,000089,000138,000530,000930,000190,000688,000892,000794,000576,000416,000569,000880,000757,000115,000024,000503,000697,000037,000821,000079,000080,000771,000252,000050,000717,000170,000120,000339,000856,000732,000200,000775,000360,000820,000001,000424,000685,000661,000406,000772,000828,000506,000751,000513,000674,000487,000949,000021,000687,000623,000813,000370,000722,000271,000895,000893,000077,000939,000210,000744,000491,000541,000613,000784,000711,000446,000575,000093,000529,000616,000925,000797,000019,000832,000871,000126,000467,000705,000665,000852,000788,000637,000609,000341,000955,000868,000470,000536,000598,000134,000641,000854,000796,000918,000189,000409,000848,000180,000286,000486,000881,000314,000040,000398,000658,000645,000545,000386,000774,000550,000900,000109,000919,000589,000305,000500,000413,000272,000455,000873,000335,000450,000160,000869,000363,000425,000205,000275,000408,000029,000082,000316,000058,000182,000537,000479,000552,000655,000246,000438,000723,000270,000365,000680,000706,000112,000627,000515,000412,000008,000224,000603,000347,000578,000642,000759,000207,000842,000648,000725,000149,000068,000755,000208,000497,000556,000028,000910,000293,000733,000061,000142,000886,000811,000608,000808,000668,000736,000400,000749,000197,000669,000199,000526,000664,000176,000148,000428,000795,000439,000277,000601,000421,000531,000897,000267,000517,000618,000654,000095,000255,000254,000192,000127,000242,000657,000673,000622,000734,000849,000905,000146,000712,000174,000932,000340,000009,000175,000201,000495,000543,000395,000768,000592,000490,000472,000579,000259,000204,000514,000122,000468,000805,000451,000945,000349,000901,000703,000244,000708,000891,000845,000287,000216,000666,000822,000285,000387,000098,000827,000474,000312,000626,000567,000958,000327,000802,000202,000385,000504,000300,000621,000410,000532,000672,000372,000103,000002,000502,000049,000539,000228,000954,000934,000651,000195,000265,000684,000018,000393,000117,000268,000303,000525,000743,000311,000599,000326,000551,000328,000352,000465,000885,000067,000086,000840,000518,000331,000355,000917,000617,000173,000381,000169,000118,000384,000602,000069,000887,000217,000159,000167,000166,000184,000211,000936,000407,000358,000604,000853,000894,000356,000916,000368,000155,000643,000048,000670,000443,000010,000215,000445,000121,000844,000635,000130,000915,000348,000289,000052,000739,000072,000063,000110,000865,000231,000776,000718,000440,000085,000760,000108,000237,000114,000877,000838,000282,000753,000931,000816,000726,000223,000116,000101,000675,000484,000485,000909,000307,000278,000388,000321,000076,000297,000798,000625,000235,000547,000227,000741,000872,000628,000920,000399,000478,000057,000758,000903,000353,000071,000570,000257,000859,000779,000436,000158,000847,000879,000471,000729,000750,000459,000264,000707,000790,000310,000814,000332,000047,000243,000429,000876,000245,000691,000615,000382,000730,000198,000320,000738,000453,000236,000633,000583,000709,000188,000476,000553,000765,000288,000136,000319,000290,000837,000632,000652,000866,000769,000762,000144,000458,000251,000682,000702,000317,000151,000899,000220,000577,000875,000375,000809,000544,000800,000106,000123,000689,000857,000380,000597,000181,000447,000806,000131,000804,000330,000663,000560,000152,000262,000414,000383,000677,000640,000055,000178,000870,000947,000042,000111,000043,000935,000943,000226,000860,000638,000325,000580,000629,000315,000620,000053,000819,000031,000183,000714,000740,000619,000938,000818,000036,000563,000017,000482,000834,000483,000761,000521,000276,000405,000662,000354,000554,000752,000364,000266,000448,000593,000437,000194,000346,000863,000913,000631,000133,000649,000362,000004,000659,000650,000596,000896,000803,000119,000333,000065,000692,000481,000239,000177,000041,000574,000032,000359,000488,000430,000156,000829,000088,000815,000716,000390,000792,000044,000929,000394,000724,000564,000411,000343,000435,000817,000334,000528,000475,000171,000157,000786,000185,000799,000313,000128,000926,000783,000234,000735,000013,000922,000696,000660,000003,000014,000630,000345,000066,000150,000464,000937,000376,000292,000007,000557,000279,000509,000824,000683,000787,000489,000147,000728,000396,000250,000928,000519,000573,000056,000105,000374,000035,000624,000851,000566,000565,000457,000810,000789,000693,000350,000193,000902,000432,000222,000582,000906,000825,000864,000302,000221,000219,000240,000025,000921,000397,000927,000778,000046,000610,000498,000600,000587,000378,000690,000678,000473,000558,000561,000011,000754,000839,000249,000667,000586,000218,000951,000823,000914,000373,000064,000748,000855,000737,000747,000129,000442,000911,000542,000125,000956,000524,000835}.tar\n"
     ]
    }
   ],
   "source": [
    "def create_train_test(path_to_data, train_perc, test_perc):\n",
    "    random.seed(42)\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path_to_data):\n",
    "        files.extend(filenames)\n",
    "    \n",
    "    saturated = files[:-1] #-1\n",
    "    unsaturated = files[-1] #-1\n",
    "    \n",
    "    dataset = wds.WebDataset(path_to_data + \"/\" + unsaturated)\n",
    "    counter = 0\n",
    "    for data in dataset:\n",
    "        counter += 1\n",
    "    \n",
    "    total_files = counter + len(saturated) * 10000\n",
    "    training_data = total_files * train_perc //10000\n",
    "    test_data_files = total_files * test_perc //10000\n",
    "\n",
    "    training_data = random.sample(files, int(training_data))\n",
    "    test_data = [file for file in files if file not in training_data]\n",
    "    test_data = random.sample(test_data, int(test_data_files))\n",
    "    # Get sample sizes of train and test data\n",
    "    training_samples = 0\n",
    "    testing_samples = 0\n",
    "    \n",
    "    for path in training_data:\n",
    "        if path in saturated:\n",
    "            training_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            training_samples += counter\n",
    "            \n",
    "    for path in test_data:\n",
    "        if path in saturated:\n",
    "            testing_samples += 10000\n",
    "        elif path in unsaturated:\n",
    "            testing_samples += counter\n",
    "            \n",
    "            \n",
    "    # Convert to filename lists \n",
    "    training_filepath = []\n",
    "    for dat in training_data:\n",
    "#        print(dat[6:12])\n",
    "        training_filepath.append(dat[6:12])\n",
    "    training_path = path_to_data + \"shard-\" + \"{\" + \",\".join(training_filepath) + \"}\" + \".tar\"\n",
    "    print(training_path)\n",
    "    \n",
    "    testing_filepath = []\n",
    "    for dat in test_data:\n",
    "        testing_filepath.append(dat[6:12])\n",
    "    testing_path = path_to_data + \"shard-{\" + \",\".join(testing_filepath) +\"}.tar\"\n",
    "    train_data = wds.WebDataset(training_path).shuffle(30000, initial=30000).decode(\"rgb\").rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\", key='__key__').to_tuple(\"image\", \"forcing\", \"lst\", \"key\")\n",
    "    test_data = wds.WebDataset(testing_path).decode(\"rgb\").shuffle(30000, initial=30000).rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\", key='__key__').to_tuple(\"image\", \"forcing\", \"lst\", \"key\")\n",
    "#    print(testing_path)\n",
    "    \n",
    "    all_filepath = []\n",
    "    for dat in saturated:\n",
    "#        print(dat[6:12])\n",
    "        all_filepath.append(dat[6:12])\n",
    "    all_path = path_to_data + \"shard-\" + \"{\" + \",\".join(all_filepath) + \"}\" + \".tar\"\n",
    "#    print(all_path)\n",
    "    all_data = wds.WebDataset(all_path).decode(\"rgb\").rename(image=\"image.pyd\", forcing=\"forcing.pyd\", lst = \"lst.pyd\", key='__key__').to_tuple(\"image\", \"forcing\", \"lst\", \"key\")\n",
    "   \n",
    "    \n",
    "    return (train_data, training_samples), (test_data, testing_samples),all_data\n",
    "    \n",
    "(train_data, training_samples_len), (test_data, testing_samples_len), all_data = create_train_test(PATH_TO_DATA, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e8d1bd-43b7-422d-b3fc-59504490d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataloader):\n",
    "    img_channels_sum, img_channels_squared_sum, forcing_channels_sum, forcing_channels_squared_sum, lst_channels_sum, lst_channels_squared_sum, num_batches = 0, 0, 0, 0, 0, 0, 0\n",
    "    for img, forcing, lst in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        img[:,:5,] = torch.clip(img[:,:5,], min=0, max=1)\n",
    "        image[:,5:7,] = torch.clip(image[:,5:7,], min=-1, max=1)\n",
    "#        img[:,-1,] = torch.clip(img[:,-1,], min=0, max=600)\n",
    "        \n",
    "        img_channels_sum += torch.mean(img, dim=[0,2,3])\n",
    "        img_channels_squared_sum += torch.mean(img**2, dim=[0,2,3])\n",
    "        \n",
    "        forcing_channels_sum += torch.mean(forcing, dim=[0])\n",
    "        forcing_channels_squared_sum += torch.mean(forcing**2, dim=[0])\n",
    "        lst_channels_sum += torch.mean(lst, dim=[0])\n",
    "        lst_channels_squared_sum += torch.mean(lst**2, dim=[0])\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    img_mean = img_channels_sum / num_batches\n",
    "    forcing_mean = forcing_channels_sum / num_batches\n",
    "    lst_mean = lst_channels_sum / num_batches\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    img_std = (img_channels_squared_sum / num_batches - img_mean ** 2) ** 0.5\n",
    "    forcing_std = (forcing_channels_squared_sum / num_batches - forcing_mean ** 2) ** 0.5\n",
    "    lst_std = (lst_channels_squared_sum / num_batches - lst_mean ** 2) ** 0.5\n",
    "    \n",
    "\n",
    "    return img_mean, img_std, forcing_mean, forcing_std, lst_mean, lst_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c948c60-1230-431d-a75d-1c3664f2a0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.8218e-01,  1.4804e-01,  1.0894e-01,  2.6186e-01,  2.5452e-01,\n",
       "         -9.3340e-03,  9.4021e-02,  4.1459e+02], dtype=torch.float64),\n",
       " tensor([5.6672e-02, 4.0446e-02, 3.3400e-02, 6.2702e-02, 7.1029e-02, 5.7446e-02,\n",
       "         7.2435e-02, 8.9265e+01], dtype=torch.float64),\n",
       " tensor([3.6346e+02, 8.3282e+02, 5.5716e-05, 9.6421e+04, 5.2780e-03, 3.0339e+02,\n",
       "         2.7644e+00]),\n",
       " tensor([6.5498e+01, 1.7130e+02, 3.7665e-04, 1.0908e+03, 2.9168e-03, 8.6245e+00,\n",
       "         1.5957e+00]),\n",
       " tensor(315.1010, dtype=torch.float64),\n",
       " tensor(10.9206, dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(all_data, batch_size=1024, num_workers=8)\n",
    "get_mean_and_std(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7e02a0-7923-4786-9ac4-ca2caf1a1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image, forcing, lst, key):\n",
    "    image, forcing, lst= image.to(DEVICE).to(torch.float32), forcing.to(DEVICE), lst.to(DEVICE)\n",
    "    \n",
    "    # Image Transformations\n",
    "    image[:,7,] = torch.clip(image[:,7,], min=0, max=600)\n",
    "    image[:,:5,] = torch.clip(image[:,:5,], min=0, max=1)\n",
    "    image[:,5:7,] = torch.clip(image[:,5:7,], min=-1, max=1)\n",
    "    image = image_normalize(image)\n",
    "    image = image[:,:7,:] #np.array([5,6])\n",
    "#    image = image_rotate(image)\n",
    "    # Forcing Transformation\n",
    "    forcing = torch.div(torch.sub(forcing, forcing_mean), forcing_std).to(torch.float32)\n",
    "#    forcing = forcing.unsqueeze(2).unsqueeze(3)\n",
    "#    forcing = forcing.repeat(1,1,33,33)\n",
    "    # LST Transformation\n",
    "#     lst = torch.div(torch.sub(lst, lst_mean), lst_std).to(torch.float32).view(-1, 1)\n",
    "    lst = lst.view(-1, 1).to(torch.float32)\n",
    "    month = tuple(int(x[2:4])-1 for x in key)\n",
    "    month = torch.tensor(month).to(DEVICE).to(torch.int64)\n",
    "    return image, forcing, lst, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e3cf51-a6c0-4c63-ab48-5ebe241e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "#        out = F.dropout2d(out, p=0.3)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "#        out = F.dropout2d(out, p=0.1)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class SE_Block(nn.Module):\n",
    "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, r=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        # add SE block\n",
    "        self.se = SE_Block(planes, r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # add SE operation\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7645be-f539-4161-93aa-b2214e019914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers: List[int],\n",
    "        in_channel=7,\n",
    "        forcing_shape = 7,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.flatten_shape = None\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channel, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool1 = nn.AvgPool2d(2)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "#        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "#        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "#        self.fc0_1 = nn.Linear(128, out_features=256)\n",
    "#        self.fc0_2 = nn.Linear(256, out_features=1)\n",
    "        self.fc1 = nn.Linear(128+forcing_shape+12, out_features=256)\n",
    "        self.drop1 = nn.Dropout(0.02)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block,\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor, forcing, one_hot_mon) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "#        x = self.conv0(x)\n",
    "#        x = x.permute([0,3,1,2])\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "#        x = self.layer3(x)\n",
    "#        x = self.layer4(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "#        x = self.drop1(x)\n",
    "        x = F.leaky_relu(x)  \n",
    "#        x = F.leaky_relu(x)\n",
    "#        x = self.fc0_1(x)\n",
    "#        x = F.leaky_relu(x)\n",
    "#        x = self.fc0_2(x)\n",
    "        x = torch.cat((x, forcing, one_hot_mon), dim=1)\n",
    "#        x = self.fc0(x)      \n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)        \n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet_simplified():\n",
    "    return ResNet(BasicBlock,[3,3,0,0])\n",
    "def resnet_bottleneck():\n",
    "    return ResNet(Bottleneck,[3,3,0,0])\n",
    "def se_resnet():\n",
    "    return ResNet(SEBasicBlock,[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b0e7ba9-6fc3-4466-a2ce-406b52d88f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# url_resnet18= \"https://download.pytorch.org/models/resnet18-5c106cde.pth\"\n",
    "# def load_pretrained_weights(model,url,mode):\n",
    "#     if mode == 'online':\n",
    "#         \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "#         state_dict = torch.utils.model_zoo.load_url(url)\n",
    "#         state_dict.pop(\"fc.weight\")\n",
    "#         state_dict.pop(\"fc.bias\")\n",
    "#         weight = state_dict['conv1.weight'].clone()\n",
    "#         state_dict.pop(\"conv1.weight\")\n",
    "#         model.load_state_dict(state_dict, strict=False)\n",
    "#         model.conv1.weight.data[:, :3] = weight\n",
    "#         model.conv1.weight.data[:, 3] = torch.mean(model.conv1.weight.data[:, :3],dim=1)\n",
    "#     elif mode == 'local':\n",
    "#         new_state_dict = OrderedDict()\n",
    "#         state_dict = torch.load('resnet_10.pt')\n",
    "#         for k, v in state_dict.items():\n",
    "#             name = k[7:] # remove `module.`\n",
    "#             new_state_dict[name] = v\n",
    "#         # load params\n",
    "#         model.load_state_dict(new_state_dict)\n",
    "# #    print(res.missing_keys)\n",
    "# #    assert set(res.missing_keys) == {\"fc.weight\", \"fc.bias\"}, \"issue loading pretrained weights\"\n",
    "#     print(f\"Loaded pretrained weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7587e52a-d137-423d-a290-00aeb4805348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=resnet_simplified()\n",
    "#load_pretrained_weights(model,url_resnet18,mode='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a529f836-ed9e-4c92-8a56-ca42b5b08662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Freeze imagenet parameters\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad and 'layer' in name:\n",
    "# #        print(name)\n",
    "#         param.requires_grad = False\n",
    "# model.bn1.weight.requires_grad = False\n",
    "# model.bn1.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557264e-0875-4e74-ab30-c1eaa4f75afd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6553 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** EPOCH: [0/100] LR: 0.001 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:13, 13.29it/s, train_loss=1.53]                          \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.5412595120961525, Test Loss: 1.5600527008034177 ------\n",
      "****** EPOCH: [4/100] LR: 0.000849 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:09, 13.38it/s, train_loss=1.43]                          \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.4265334830551777, Test Loss: 1.5252548123069698 ------\n",
      "****** EPOCH: [11/100] LR: 0.000638 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:09, 13.39it/s, train_loss=1.39]                          \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.4029472951957407, Test Loss: 1.4806861467739245 ------\n",
      "****** EPOCH: [14/100] LR: 0.000565 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:11, 13.34it/s, train_loss=1.34]                          \n",
      "6556it [08:06, 13.47it/s, train_loss=1.44]4.29it/s, train_loss=1.12]\n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.3928571643445316, Test Loss: 1.4949305130276487 ------\n",
      "****** EPOCH: [17/100] LR: 0.0005 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:19, 13.13it/s, train_loss=1.34]                          \n",
      "6556it [08:18, 13.15it/s, train_loss=1.25]                          \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Loss\n",
      "------ Train Loss: 1.3700245755194453, Test Loss: 1.653372911875968 ------\n",
      "****** EPOCH: [23/100] LR: 0.000391 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:13, 13.29it/s, train_loss=1.25]                          \n",
      "6556it [08:08, 13.42it/s, train_loss=1.27]                           \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.3322730180946627, Test Loss: 1.4484827891151706 ------\n",
      "****** EPOCH: [27/100] LR: 3.3e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.3158776668457812, Test Loss: 1.4442790359960633 ------\n",
      "****** EPOCH: [30/100] LR: 2.9e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:07, 13.45it/s, train_loss=1.24]                           \n",
      "6556it [08:10, 13.36it/s, train_loss=1.24]                           \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.309526283541709, Test Loss: 1.4409810803687035 ------\n",
      "****** EPOCH: [34/100] LR: 2.5e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.3074831864759644, Test Loss: 1.4396025258928091 ------\n",
      "****** EPOCH: [36/100] LR: 2.3e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         34] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.3056529073489012, Test Loss: 1.4380709107733896 ------\n",
      "****** EPOCH: [38/100] LR: 2.1e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.3025503157205593, Test Loss: 1.436157810483806 ------\n",
      "****** EPOCH: [42/100] LR: 1.8e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.3013255152217202, Test Loss: 1.435245228783732 ------\n",
      "****** EPOCH: [44/100] LR: 1.7e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:19, 13.13it/s, train_loss=1.21]                           \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         36] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.2994237882178903, Test Loss: 1.4333185776908597 ------\n",
      "****** EPOCH: [48/100] LR: 1.4e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [08:18, 13.16it/s, train_loss=1.21]                           \n",
      "  0%|          | 0/6553 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Loss\n",
      "------ Train Loss: 1.2986419588654705, Test Loss: 1.432416148946423 ------\n",
      "****** EPOCH: [50/100] LR: 1.3e-05 ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 336/6553 [00:33<07:18, 14.17it/s, train_loss=1.38] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "DECAY_RATE = 0.96\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model = torch.nn.DataParallel(model)\n",
    "loss_fn = nn.SmoothL1Loss() #\n",
    "#loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=DECAY_RATE)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=5)\n",
    "\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "lst_mean = lst_mean.to(DEVICE)\n",
    "lst_std = lst_std.to(DEVICE)\n",
    "forcing_mean = forcing_mean.to(DEVICE)\n",
    "forcing_std = forcing_std.to(DEVICE)\n",
    "\n",
    "min_test_loss = np.inf \n",
    "torch.backends.cudnn.benchmark = True\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"****** EPOCH: [{}/{}] LR: {} ******\".format(epoch, EPOCHS, round(optimizer.param_groups[0]['lr'], 6)))\n",
    "    running_train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    running_test_loss = 0\n",
    "    test_n_iter = 0\n",
    "    \n",
    "    # model.train()\n",
    "    loop_train = tqdm(train_loader, total=(training_samples_len//BATCH_SIZE) + 1, leave=True)\n",
    "    for idx, (image, forcing, lst, key) in enumerate(loop_train):\n",
    "        image, forcing, lst, month = process_data(image, forcing, lst, key)\n",
    "        one_hot_mon = F.one_hot(month, num_classes=12)\n",
    "        optimizer.zero_grad()\n",
    "        forward_out = model.forward(image, forcing, one_hot_mon)\n",
    "        loss = loss_fn(forward_out, lst)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "        loop_train.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    loop_test = tqdm(test_loader, total=(testing_samples_len//BATCH_SIZE) + 1, leave=False)\n",
    "    \n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, forcing, lst, key) in enumerate(loop_test):\n",
    "            image, forcing, lst, month = process_data(image, forcing, lst, key)\n",
    "            one_hot_mon = F.one_hot(month, num_classes=12)\n",
    "            pred = model.forward(image, forcing, one_hot_mon)\n",
    "            testloss = loss_fn(pred, lst)\n",
    "            running_test_loss += testloss.item()\n",
    "            test_n_iter += 1\n",
    "            loop_test.set_postfix(test_loss=testloss.item())\n",
    "\n",
    "    avg_train_loss = running_train_loss/train_n_iter\n",
    "    train_loss.append(avg_train_loss)\n",
    "    avg_test_loss = running_test_loss/test_n_iter\n",
    "    test_loss.append(avg_test_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    scheduler2.step(avg_test_loss)\n",
    "    if avg_test_loss < min_test_loss:\n",
    "        print(\"Saving Model\")\n",
    "        min_test_loss = avg_test_loss\n",
    "        torch.save(model.state_dict(), \"resnet_dropna_6.pt\")\n",
    "    \n",
    "    print(\"Saving Loss\")\n",
    "    file_name = \"train_loss_dropna_6.pkl\"\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(train_loss, open_file)\n",
    "    open_file.close()\n",
    "    file_name = \"test_loss_dropna_6.pkl\"\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(test_loss, open_file)\n",
    "    open_file.close()\n",
    "    print(\"------ Train Loss: {}, Test Loss: {} ------\".format(avg_train_loss, avg_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d95398-5efd-4cac-8232-43772a3b35d6",
   "metadata": {},
   "source": [
    "# Igonre stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741573ab-edee-4dc7-b552-ae4077af7f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"train_loss_dropna_3.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "train_loaded = pickle.load(open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_dropna_3.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "test_loaded = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175b03f9-0244-4fbc-9bf9-4fbdb3563b3d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.623094824447877,\n",
       " 1.5535237804470021,\n",
       " 1.523772669170347,\n",
       " 1.5067520003757844,\n",
       " 1.4975652059089448,\n",
       " 1.494133774550109,\n",
       " 1.4947242433049694,\n",
       " 1.4917582646171847,\n",
       " 1.476196739096713,\n",
       " 1.4740029177778027,\n",
       " 1.4691677824843363,\n",
       " 1.4690041492276324,\n",
       " 1.467400571901742,\n",
       " 1.4679920830359041,\n",
       " 1.4683060339737755,\n",
       " 1.4677135049658618,\n",
       " 1.4686041538158883,\n",
       " 1.4684076065424934,\n",
       " 1.4687736870389934,\n",
       " 1.458025435053562,\n",
       " 1.4566276312129656,\n",
       " 1.4558493085285327,\n",
       " 1.4554497317532655,\n",
       " 1.455107504346386,\n",
       " 1.4546225233047372,\n",
       " 1.454252869209845,\n",
       " 1.4539630859771173,\n",
       " 1.4537511470486317,\n",
       " 1.4535952553759295,\n",
       " 1.4534635915235483,\n",
       " 1.4534805330270364,\n",
       " 1.453465586811368,\n",
       " 1.453316579921853,\n",
       " 1.4531668945349259,\n",
       " 1.4529479568469192,\n",
       " 1.4525124650185317,\n",
       " 1.4521347718269462,\n",
       " 1.4517622685789806,\n",
       " 1.4513494400447136,\n",
       " 1.4509334125151216,\n",
       " 1.4505654322003398,\n",
       " 1.4503354633135255,\n",
       " 1.450055730929977,\n",
       " 1.4496734563492606,\n",
       " 1.4492970948811517,\n",
       " 1.4488811457182662,\n",
       " 1.4484669729524813,\n",
       " 1.448075977586832,\n",
       " 1.4476071395761707,\n",
       " 1.4471369975384094,\n",
       " 1.4466039049548818,\n",
       " 1.4461031573998087,\n",
       " 1.4455233817437476,\n",
       " 1.4449816641000914]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc184203-2848-41ad-adf4-35dadc19048d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"train_loss_dropna_6.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "train_loaded_2 = pickle.load(open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_dropna_6.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "test_loaded_2 = pickle.load(open_file)\n",
    "open_file.close()\n",
    "test_loaded_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cb713e-dc99-44b7-8e59-27306fd65279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABC9klEQVR4nO3dd3hUVfrA8e+5M5NeCCkQuiidRJoosAgILqKsKCK6ylpWFwXUxXVd+VlWXRuWXV0biKsiC+iiorKKFF2KBQsIJAQSQgklkARI71PO7487GRJIZ0KS8f08z31uv/edS3jPnXPPnKu01gghhPBdRnMHIIQQomlJohdCCB8niV4IIXycJHohhPBxkuiFEMLHSaIXQggfV2eiV0p1VkqtU0rtVEolKaX+WM02Sin1slJqj1IqQSk1qGnCFUII0VDWemzjAO7TWv+slAoFtiil1mqtd1baZgLQwz1cCMxzj4UQQjSzOu/otdZHtdY/u6cLgF1Ax1M2mwQs0qbvgTZKqVivRyuEEKLB6nNH76GU6gYMBH44ZVVH4FCl+cPuZUdP2X86MB0gODh4cO/evRsYLmQXF3O0ZD8xgR2IDopo8P5CCNGabdmy5bjWOroh+9Q70SulQoCPgNla6/yGBgegtV4ALAAYMmSI3rx5c4OP8Z+tW3ky4Sam93mQu4f+tjFhCCFEq6WUOtDQferV6kYpZcNM8ku01sur2SQd6FxpvpN7mddZLWbZ5NTOpji8EEL4nPq0ulHAW8AurfU/athsBXCTu/XNRUCe1vpoDdueEauyAOB0SaIXQoj6qE/VzQjgd0CiUmqbe9mDQBcArfV8YCVwObAHKAZu9XqkbjaLmegdkuiFEKJe6kz0WutvAFXHNhqY5a2gamNRkuhF61ZSUsK+fftwOuVvWNTMYrHQvXt3AgMDz/hYDWp10xL4Wd1VN9rVzJEI0Tj79u0jKiqK6OhoDEN+nC5O53K5OHbsGPv27aNfv35nfLxW91dmkTp60co5nU5J8qJWhmEQHR3ttW99re4vzeZpdSN39KL1kiQv6uLNv5FW99dmdX94p0sSvRBC1EcrTvRSdSOEEPXR6hK9TX4wJcQZOX78OM8++2yD9xs1ahTHjx9v8H5Tpkxh4cKFDd6vuW3atIkPPvjAM3/ffffx6KOPNmNEjdf6Er0hrW6EOBMnTpzgX//612nL7XZ7rftt2LCBqKiopgqrxdm8eTOfffZZc4fhFa2ueWXFD6bkjl74gr98uJ2UjAKvHrNX+1Cem3J+jevvu+8+Dh06RO/evbFarQQEBBAeHs7evXtJS0vj0ksv5ciRI5SVlTFjxgzuu+8+ADp27MjmzZvJz89nwoQJDB06lM2bN9O+fXtWr15NcHBwnbGtWLGCBx54AKfTyYABA3j33XcJDAxk1qxZrFq1CovFwpgxY3jjjTd45513ePrppzEMg9DQUGrqG+uVV15hxYoVFBcXk5aWxl133UVZWRn/+c9/8PPzY+3atcTExLBp0ybuvPNOSkpK6NatG0uWLCE6OpqhQ4cyZMgQvvnmG/Lz83njjTcYNWoUTz/9NKWlpfTu3Zv7778fgF27djF06FCOHDnCjBkzeOihhxrxL3T2tbo7eouh0NqQOnohGunvf/87nTt3Jjk5meeee46kpCRee+010tLSAFiyZAlJSUls376d+fPnk5mZedoxDh48yB//+Ef27NlDWFgYixYtqvO8xcXFTJ8+nWXLlrF7924cDgcvvPACmZmZrFy5ktTUVHbv3s1TTz0FwDPPPMOaNWtISUlh1apVtR47JSWFzz//nJ9++omnn36a4OBgdu3axQUXXMAbb7wBwM0338zcuXPZvXs3ffv2Zc6cOZ79HQ4HCQkJPP/88zz++OMEBATw4IMPcuWVV5KcnMxtt90GQGpqKhs2bOCnn37i+eefp6ysrF7XvLm1ujt6QynQSqpuhE+o7c77bImPj6dyl+HPPvusp8oiIyODpKQk2rVrV2Wfjh07MmzYMAAGDhzI/v376zxPQkICnTp1Ii4uDoBbbrmFV199lTlz5uDn58f111/PxIkTmTp1KgAXXHAB06ZN45prruHGG2+s9djDhw+nTZs2tGnThpCQEKZMmQJAXFwcCQkJnDhxgoKCAiZMmADA9OnTPdsAXHvttQAMGzbM8w2mOuPHjycwMJDAwEDatm1Leno63bt3r/OzN7dWeUcPBi5pXimEVwQFBXmmV65cyfr169m8eTMpKSn06dOH0tLS0/bx8/PzTFut1jP6YY/NZmPbtm1ce+21fPbZZ4wePRowv1k8+eSTHDx4kEGDBlX7zaKCv7+/Z9owDAICAjzTDoejzhgqtrdYLLV+lsrnsVgsdT7XaClaaaJXUkcvRCO1adOGoqKiatfl5uYSHh5OaGgo27ZtY/v27V47b3x8POnp6SQlJQGwaNEiLr74YvLy8sjOzubaa69l3rx5JCcnA7Bz507GjBnDSy+9REREBPv27Wv0uSMjIwkLC2P16tUAvPXWWwwfPrzWfcLCwigsLGz0OVuSVlt145KqGyEapV27dgwZMoQePXrg7+9PdPTJlxVdffXVzJ8/n+7du3Puuedy/vneq1oKCgrijTfeYMqUKZ6Hsffddx9ZWVn85je/8XxzePLJJwGYPXs2aWlpaK0ZOXIkF154Zq+hXrhwIXfeeSd33303Xbt2ZenSpbVuP2HCBJ5//vkqD2NbK2V2PHn2NfYNU9lF5Vz8n18xsO0l/PuquU0QmRBNKyEhgfj4+OYOQ7QC1f2tKKW2aK2HNOQ4rbLqRmtD7uiFEKKeWl3VTUUdvSR6IVqWm266iR9//LHKspkzZ3LPPfd45fjLly/nwQcfrLKsS5curFmzxivH92WtL9ErBdqQh7FCtDD1aUt/JiZPnszkyZOb9By+qtVV3Zh9mskdvRBC1FerS/RyRy+EEA3T+hK9u46+uVoLCSFEa9PqEr1S7l/Gyh29EELUS6tL9ID8YEqIM9DY/ugBnnjiCQoKau9ts2PHjhw9erRRx29Oixcv5ueff/bMDx06lK+//roZI/Ke1pnoMXAhd/RCNEZN/dHXx/z5832mW4BTffLJJyQkJDR3GE2i1TWvBFDSqZnwFZ/Ogsyd3j1mu74w6bUaV1fuj3706NHExMSwfPlyysvLueKKK3jxxRfJz8/nyiuv5OjRo7hcLubMmUNGRgZZWVmMGjWKiIgIfvjhhzpDeeyxx1iyZAlgtrN/5JFHqj32bbfdVm2f9NWZMmUKAQEBJCYmcuLECRYsWMDChQvZvHkzgwYN4sMPPwRgwYIFvPDCC2itufTSS3n99dcBsyuG22+/ndWrVxMQEMDnn39OcnIya9euZdOmTcydO5fly5cD8N577zFr1ixPP/Xjx49v0D9FS9F6Ez2S6IVojL///e9MnDiR5ORkPv74Y5YtW0ZCQgJaa8aNG8eqVavIzMykffv2rF+/HjC/BURGRvL666+zYcMGYmNj6zzPN998w9KlS9myZQtaawYPHszYsWNJTU097dgVfdLv3bsXwzDqfGVhbm4uW7du5f3332fq1Kn873//Y/DgwcTHx7Np0yZiY2N59NFH2bJlC9HR0YwcOZLFixczbdo0SkpKGDZsGC+//DIzZszg5Zdf5rnnnuPSSy9l4sSJ3HLLLZ7zVPRT/8EHH/D4449Loj+7pAsE4SNqufM+G1atWsXGjRvp27cvYL4cJDk5mbFjx/Lggw8yc+ZMJk2a1KgEt379ei6//HLCwsIAmDhxIuvWrePKK6887dh2u73aPulrMnHiRAzDYODAgURGRjJ06FAAevXqxd69e9m/fz/Dhg2jQ4cOAFx//fVs2LCBadOmYbPZuO666wAYMmRIrb+srW8/9S1dq6yjVyipoxfCC7TW3HvvvSQnJ5OcnMzBgweZPXs2cXFxbN26lbi4OB5++GGv9t5Y3bFr6pO+JpX7m6/cN359+p+3Wq0Y5i8v6+x/vr791Ld0rTLRgyHt6IVopMr90U+YMIFFixaRl5cHwP79+0lPTyctLY2QkBDPO2O3bdsGQHBwsGfbuowZM4aVK1dSUFBAfn4+n3/+OWPGjKn22DX1Sd9YI0aM4Pvvv+fo0aM4HA6WLVvGmDFjat0nJCSE/Pz8MzpvS9Uqq24UBlqqboRolMr90Y8dO5apU6dywQUXAOaDyqVLl5KcnMycOXMwDAOr1cq8efMA872rEyZMICYmps6HsSNGjOCGG25g0KBBgPkwdvjw4Sxfvvy0Y+fm5lbbJ31jde3alccee4xRo0Z5HsbecMMNte5z4403cueddzJv3jw++uijMzp/S9Pq+qMHGLDgasICrWz83QdejkqIpif90Yv6+sX2Rw/uOnq5oxdCiHppnVU3ykBT9wt/hRBNJz4+nvLy8irLFi1a5GkBc6bmzJnDJ598UmXZVVddxdy58ma5hmqViR4MtLSjF6JZNfWvSOfOnStJ3UtaZdWNIe3ohRCi3lploldyRy+EEPVWZ6JXSr2tlMpSSu2oYX24Uuq/SqntSqkkpdSt3g/z1HNKohdCiPqqzx39QuCyWtbPAnZqrc8HRgN/V0r51bL9GVPygykhhKi3OhO91nojkF3bJkCoMt8IEuLetkmbxEjVjRCN19j+6EeNGlVnZ2PVmTJlCgsXLmzwft5WUxwrV66s9Vez9913H48++uhpy1NSUujRo0ejYjnbffZ7o47+VaAPcARIBP6oa/jZqlJqulJqs1Jq87Fjxxp9QkOqboRotJr6o7fb7bXut2HDBqKiopoqLNGEvNG8cjywDbgEOBdYq5T6Wmt9WqcRWusFwAIwfxnb2BPKHb3wFX/99q+k5qR69Zg9InrwtxF/q3F95f7orVYrAQEBhIeHs3fvXtLS0rj00ks5cuQIZWVlnv5owLwL3bx5M/n5+UyYMIGhQ4eyefNm2rdvz+rVqwkODq4zthUrVvDAAw/gdDoZMGAA7777LoGBgdX2Rf/OO+/w9NNPYxgGoaGh1PRL+pSUFG688UaKi4sBePnllxk3bhwul4tbb72VjRs3Ehsbi81m8+zz0Ucf8ec//5nAwEAuvPDCOuNOSEhgwIAB5OTk8Mc//pE//elPVdYXFxdz0003kZCQgMVi4fnnn2fixIk4HA5mzZrF//73P5RS3HzzzTz00EOe/YqKipgwYQJXXXXVacf0Jm8k+luBudqsNN+jlNoP9AZ+9MKxq2Uo6etGiMaq3B/9ypUrueaaa9i6dSu9e/cGYMmSJcTExFBUVMSAAQOYNm0a7dq1q3KMgwcPsmTJEoYNG8bll1/OokWLmDFjRq3nLS4uZvr06axdu5a4uDgmT57MCy+8wPTp06vti/6ZZ55hzZo1nHPOObVWGXXo0IGNGzcSFBTEjh07uP7669mxYweLFy9mz549pKamcvjwYeLi4rj11lspLi5m1qxZfPnll/Tt25eJEyfWec127tzJzz//TH5+PgMHDmTy5MlV1j/33HMYhsHu3bvZtm0bEyZMYO/evbz66qscOHCAnTt3YrPZyMrK8uyTn5/PNddcw4033sisWbPqjOFMeCPRHwTGAl8rpdoBvYB9XjhujZQyQO7ohQ+o7c77bImPj/ckeYBnn32Wzz77DICMjAySkpJOS/QdO3Zk2LBhAAwcOJD9+/fXeZ6EhAQ6depEXFwcALfccguvvvoqc+bMqbYv+gsuuIBp06Z5kmFNysvLuf3220lKSsIwDNLS0gCzqmnq1KlYrVa6devmiXf79u106tSJ/v37AzBt2jTefPPNWmOfMGECwcHBBAcHM3z4cL755htPR3AA3333HXfffTcAAwYMoGPHjiQmJvLVV18xY8YMz7eJmJgYzz5XXnkl9957L3feeWed1+5M1ad55XvAJqCXUuqwUuo2pdSdSqmK6J4AhiulEoGvgAe01g1/YtMABhapuhHCS4KCgjzTK1euZP369WzevJmUlBT69Onj6VGyssp9wFut1jPqq72mvuiXLFnCk08+ycGDBxk0aBCZmZnV7v/0008TExPDrl27SExMrLM/+sYw25qcVNGf/ZkYOnQoq1evPiuvRa1Pq5vfaq1jtdY2rXUnrfVbWuv5Wuv57vVHtNa/1lrHaa37a60XN3nQSkmiF6KRKvdHf6rc3FzCw8MJDQ1l27ZtbN++3WvnjY+PJz09naSkJMDsF+fiiy+usS/6nTt3MmbMGF566SUiIiLYt6/6ioK8vDxiY2OxWCzMmzfPU+iMGjWKDz/8EIfDwYEDB/j+++8BOP/880lPT2fnTvNdve+9916dsX/xxRcUFxeTmZnJpk2bGD58eJX1w4cPZ/FiM/UlJiZy5MgR4uPjGTduHPPnz/c86K5cdfPcc8/Rpk0bbr755npfw8Zqnb+MVQZmq04hRENV7o/+L3/5S5V1V199NQ6Hg+7du3P//fdz/vnne+28QUFBvPHGG0yZMoWePXtiGAb33Xcfubm5TJgwgZ49ezJs2DBPX/SzZ8+mZ8+e9OjRg6FDh9b40HT27NksXbqUXr16kZycTGBgIGBWyXTv3p0ePXpwww03MHDgQE8cr776KldccQV9+/YlOjq6ztj79u3L8OHDGTp0KPfffz/dunWrsv4vf/kLTqeTnj17MnXqVBYsWEBgYCCzZ8+mc+fO9O7dm169evHWW29V2e+tt96ipKSkzucbZ6pV9kd/6bv3kun6loRbm+x5rxBNRvqjF/X1i+6P3pA7eiGEqLdW2U2xoQy0kjp6IVqSm266iR9/rPote+bMmdxzzz1eOf7y5ct58MEHqyzr0qULa9as8crxX375ZV5//fUqy4YOHcqiRYu8cvzm1DoTPRakeaUQLUtTJ8TJkyef1n7dm+655x6vFUotTSuuupFEL4QQ9dGKE73U0QshRH203kSvtHRVLIQQ9dBKE70FQF4nKIQQ9dBKE70ZtiR6IRqusf3RAzzxxBMUFBTUus3Z7mu9oXHU1L98haFDh/L111+ftvyVV15p1K9Yz6Tfem9plYne4k70Tt34/jWE+KWqqT/6+pg/fz6FhYVejkg0tdbZvFKqboSPOPLQQ5Tt9m5/9P49e9DhqadqXF+5P/rRo0cTExPD8uXLKS8v54orruDFF18kPz+fK6+8kqNHj+JyuZgzZw4ZGRlkZWUxatQoIiIi+OGHH+qM5bHHHmPJkiWA2c7+kUceqfbYt912W7V90lfnvffe45lnnsFutxMREcGyZcvo1KkTmZmZXHPNNWRkZDB48OAqz/DmzJnD+++/T2RkJB06dGDQoEG1xv3OO+/whz/8AafTyb/+9S9GjRpVZX1KSgo333wz2dnZREZGsmjRInr06MHhw4f5/e9/z4EDBwB47bXX6Ny5s2e/Xbt2MXnyZN544w0uvvjiOq+ft7TKRG8x5I5eiMaq3B/9xx9/zLJly0hISEBrzbhx41i1ahWZmZm0b9+e9evXA+a3gMjISF5//XU2bNhAbGxsnef55ptvWLp0KVu2bEFrzeDBgxk7diypqamnHTszM7PaPumrM27cOK677joMw+DFF1/kb3/7GwsWLGDOnDkMGzaM559/nv/85z8sW7bME8fy5cs9PVuef/75dSb64uJikpOTWbVqFbfffjupqVUL4xkzZjBt2jTuuusu/vnPfzJz5kzWrl3LHXfcwciRI1mzZg0Oh4O8vDzPZ0lISOC6667jnXfe4aKLLqrz+nlTq0z0ckcvfEVtd95nw6pVq9i4cSN9+/YFTia4sWPH8uCDDzJz5kwmTZrE+PHjG3zs9evXc/nllxMWFgbAxIkTWbduHVdeeeVpx7bb7dX2SV+d/fv3c/XVV5OVlYXdbqdTp04AbNq0iY8//hiA6667junTpwOwbt06Lr/8ckJDQwHq9Vkq+r+/7LLLKCwsPK3g2bp1K1988QUAd955J4899hhg9kv/4YcfAmb3zZGRkRw/fpzs7GwmTZrERx99VGch0xSkjl6IXzCtNffeey/JyckkJydz8OBBZs+eTVxcHFu3biUuLo6HH36Y+++/32vnrO7YNfVJX5277rqLmTNnsnv3bl577TXKysq8FluFU/ufP3W+oUJDQ+nYsSPr1q07o+M0VqtO9HJHL0TDVe6PfsKECSxatIi8vDzAvFtOT08nLS2NkJAQzztjt23bBkBwcLBn27qMGTOGlStXUlBQQH5+Pp9//jljxoyp9tg19UlfnYKCArp06QLA22+/7Vk+bNgw3nnnHQA+/PBD8vPN11ZfcsklrFy5kqKiInJzc+vVN05FH/Vr1qwhNDSUyMjIKusHDRrkOfeCBQs8b5saMWIEL7zwAgAOh4MTJ04A5stVVq5cydKlS2t89tCUWnXVjcPl/TfJCOHrKvdHP3bsWKZOnepJVEFBQSxdupTk5GTmzJmDYRhYrVbmzZsHwM0338yECROIiYmp82HsiBEjuOGGGzxVFTfddBPDhw9n+fLlpx07NzeX3/zmN563WVX0SV+dRx55hOuvv57w8HBGjhzJwYMHAZg7dy7XXHMN5513HkOGDPE8RxgxYgSTJ0+mX79+REZG1quP/YCAAPr06YPD4ai2hdK8efO46aabePHFFz0PY8FslXTLLbfw73//G8MweO211zxVS2FhYaxevZrRo0cTGhrKDTfcUGcc3tIq+6OfueJ1vs6ZxxdXr6ZTWAcvRyZE05L+6EV9/aL7o7e47+jtckcvhBB1apVVNxbDXXXjlDp6IZpLfHw85eXlVZYtWrSIoUOHeuX4c+bM4ZNPPqmy7KqrrmLu3LleOX5T95/fkrTORO9+GGt3Sasb0Tq5XC4Mo1V+ofZISEho0uPPnTvXa0m9Oi39hSIul/duZFvlX5pU3YjWzGKxcOzYMa/+Rxa+xeVycezYMSwWi1eO1zrv6N13Qnan3NGL1qd79+7s27ePzMzM5g5FtGAWi4Xu3bt75VitMtFbK5pXSqIXrVBgYCD9+vVr7jDEL0irrLqpqNt0yC9jhRCiTi0y0Rd99x37r51K9qJFOI4dO229Re7ohRCi3lpkotdOJ9rhIPPpZ0gdNZqDt91O7ief4Cw0f7ZtNSoexkqiF0KIurTIOvqQkSMJGTmSsj17yPvvZ+R/9hlH5/wfGQGPE3rJGDp2i8YSonFKohdCiDq1ii4QtNaUbN1K3n//S8EXq3Dm5pIfCOXXTWHEnx7B8PNr4miFEKJl8NkuEJRSBA0aROyjj9Lj6438fPdt7I1VRC38kH2XX0H+qlU0V4ElhBAtXatI9JUpm42cgXE8c52F/X+9CyMoiPTZ93Lghhsp2b69ucMTQogWp2Um+rx02PA81HCXXtHXTU7/8zjn4+W0f+JvlB8+RNp115N+35+xp6efzWiFEKJFa5mJfu9XsO5JSP6s2tWhtnC0Vryc+BjzdyzAMmk8561aRdTMGRR89RV7J1xO1t//UW3TTCGE+KVpmYn+/Bsgqhd8+Tg4T+/PpmNwV4rT7qJPm4G8vu11xn84ntd3v4PfHTdz7qovCJswgRNvvknqxaM4MO13ZC9egj0rqxk+iBBCNL86E71S6m2lVJZSakct24xWSm1TSiUppTaccVQWK4x7FE6kwrbFp602DIWrtCN393+KD3/zIcM6DOONhDf49Ye/5rX09wl4/C90X/k5UbNm4czLJfPJJ9kzajRp06aRvejf2KWPESHEL0idzSuVUhcDhcAirXX/ata3Ab4DLtNaH1RKxWit67x9rrN5pdbw9njIOQD3bAW/IM+qb1KPM+2tH/jgzmFc0K0tAKk5qbyZ8Car0lYRYA3gul7XcU2Pa+gW3o2yvXvJX7WKglWrKUtNBSBw0CCCRwwnMC6OgP79sbZtW1fIQgjR7BrTvLJe7eiVUt2Az2pI9DOBDlrrhxty4nq1oz/4vZnsL3kELv6zZ/F3e49zw5s/8N4fLmLYuVVf2rsvbx9vJrzJyv0rcWkX3cK6MabzGEZ3Hs350efjSDtAwerV5K9eQ1lKiueBr61DBwLi4giM609A//4E9OuHJTS0IR9JCCGaXHMl+pcAG9APCAX+qbWutkd/pdR0YDpAly5dBh84cKDuCN+7AdK+hnu2QbCZ1H/Yd4LrFnzPktsvZMR5UdXullGUwbpD61h/aD0/ZvyIw+WgjX8bLu50MWM6j2F4h+H4l7koTdpJ6Y5ESnbsoDRxB/bDhz3HsMbEYGnbFktEG6wRbbFERJhD2wisEREE9O2LX9eudX8GIYTwksYkem90gWAFBgNjgUBgk1Lqe6317lM31FovABaAeUdfr6OP/SvMGwZf/x0uexoAi6EAsNfyKsH2we35be/f8tvev6WwvJBvj3zL+kPrWX9oPSv2rsBm2IiPjic+Kp64cXHEXX8F5wW3x5GTQ+mOJEp3JFJ++DDO7BycOTmUHknCkZODKz/fcw7l50e3D5YR0KtXvT6KEEI0B28k+sPACa11EVCklNoInA+clugbJaY3DLgRfnoTLrwDIrrSNTKYAJvBB5sPM7pXTJ2HCPELYXy38YzvNh6Hy8HWrK1sOLSBrVlbWbxrMfYku3mqwBj6R/UnLjqO+KsG0zfyd4T4hVQ5lrbbceblYc/I5NCMO0n/032c88EyjKCg6k4thBDNzhtVN32AV4HxgB/wI3C91rrGVjrQsL5uyEuHVwZB36tg8hsAvPJVKn9fu5vFt13Ir3pUX31TH+XOclKyU0g4nkDi8UQSjyVysOCgZ327oHac1+Y8zm1zrmd8bptzCbYFU7RpEwd/fxvh10ymw5NPNjoGIYSoryapo1dKvQeMBqKATOBRzDp5tNbz3dvcD9wKuIB/aa1fquvEDUr0AGsfhW//CXd+De3jKLU7ueyljRiGYtUfL8bP6r2fBOSW5pJ4PJGUnBT25u5lb+5e9uXto8xZ5tkmNjiWwe0GM31TCKVvL6bDCy8QPvEKr8UghBDVabKHsU2hwYm+JBf+eT50GgLTPgJgfUoWt7zzEw9c1psZo89tmkDdnC4n6YXp7Mndw97cvaTmprLu4DoCtJWXlocRcvAE53y8HL8uXZo0DiHEL5vP9l4JQGAbs4nlni9hn/mbrNG9Yvh133a8/FUqR3JLmvT0FsNCl7AuXNLlEv4Q/weeu/g5PrryI86L7s2fx6RT7Cpl/+x70OXlTRqHEEI0VOtJ9AAX/AHCOsGXj3ravz8ysS8azZOf7zzr4XQJ68Jb499ixvi/8uYVfrh2prDhkTtw6ZpbAwkhxNnWuhK9LQAueQiObIWdnwDQuW0Qd405j5WJGXydevY7MTOUwdReU3lkzudsH9mBdp9+zxMvXk1aXtpZj0UIIarTuhI9QPx1ENMXvvobOMxqkj9c3J1ukUE8+mkSZY7meb1gbEgs1778GWXd2nPZ4lRuWzKZhTsWygtRhBDNrvUlesMC4x6H7H2wZAoUncDfauGxK/ux73gRb32zv9lCswQG0ue1fxHm8mfO6kD+8dMLfLr302aLRwghoDUmeoCev4ZJr5t94SwYBUe2MrpXDOP7teOVr/aQ3sQPZmvjf+65xD7yMJ1Sspm5I5Znf3yWo4VHmy0eIYRonYkeYOCN8PtV5kPZt8bDtqWeB7NPNcOD2crCJ08m7IorGPXFEa76qphHNzwkD2iFEM2m9SZ6gI6D4I4N0OVC+GQGnb77K/eM6srKxAw27m6+t0sppYh94m+EX30VV35bzqTnNvHx2pebLR4hxC9b6070AMFRMO1jGH43/PQmd6bNZlBEKY+taL4HswBGUBAdnnqKjq++QvsiG+fd+wZ75r+IdsmdvRDi7Gr9iR7MN1L9+kmY8jZGZiLvMYc2J7byp2XbKSlvvmQPEDZuHB0/XsbOc23YX1rAgVt/j/3IkWaNSQjxy+Ibib5C/2vg9i/xDwjmg4AnGbjzOe587RMO5xQ3a1jtO/cm/MVnmHe5QeH2n9k36SryVqyQppdCiLPCtxI9QLt+MH0dlvhr+b1tDW/l3U7Cy9ey/cf1zRrWhO6XY7vyMu77vcJ1TieO/OUB0v84m6JNm3CVljZrbEII39Z6OjVrjNxD5K1/Bdu2RQRRwpG2Q4m97H5Uj0tBqaY9dzVySnO4+tOrifJvy2sZl5Lz2jy03Y6y2Qg8/3yCLryQoAuHEjhgAIaf31mPTwjR8vl275VnoCD3OKsWPcuvTnxIrMrGFd0bY/jdZlWPLfCsxFBh/aH13P2/u7k97nbu6nk7JT9voeiHHyj+4UdKd+4Elwvl70/ggAEEDb0AW2wHjOBg9xCEJSSk0nwwyuqNd8e0XLq8HFdJCUZYGKoZCmchWhpJ9LVwuTSvfrmT/ev/zT2BqzjHuR8MG3QcDN1GQNcR0PlC8A+p+2Bn6JFvH2HF3hW8e9m7DIgZ4FnuzM+nePMWin/4gaIff6Rs1646j2XtEEtgXDyB8XEE9I8zX2oeEtyE0Tc9rTUl27aR9+mn5H+xCldeHlitJ9/d27Yt1rYRWCLaYmkbYb7EXRnmtzRDmQWCUu5lYI2KImTMGCkohE+QRF8Pq5My+NN/tjLSmswjvdPpmPczHNkG2gnKAh0GmEm/6wiIjYfgaLDYvBpDYXkhk1dMxs/ix+IJi2kT0Kba7ZyFhThzc3EVFZlDYeHJ6aIinAWFlO/bS0lC4smXmiuF37ndCYyLJyCuPwG9++DXrSuWiIizkuhc5eWU791LaUoKZSm7Kdu3F1u79gT060dA/34E9OiBqqFaqvzwYfJWrCDv00+xHziICggg9NJLCejTB2dODo6cbPMdvtnZ5nRObpV3+NYmbOJEYp96EsPf35sfV4izThJ9PaVmFnDHv7ew73gR11/Qmf8b25nw4z/DgW8h7VtI3wIu+8kdAttCSDsIiYbgmJPT/mGgXeBynDI4zXHFvkGREBxpjt3DjyeSuGPtHYT6hXLv4HuZdN4kDNX4Z+OO7GxKd+ygJCGR0sREShITcWZne9YboaH4de16cuhmjm2dOmFp0wZlsdT7XNrlMhPvsWPYMzIoS02lLDmFst0plO3bD06zSavy98eva1fsmZnmXTmgbDb8e/UioH8/Avv3x793H8qSd5H3yacUu/8egi68kPBJkwj99a/r/Hai7XachYXuGW0OLpfZosk95H3yKcdeeonAgQPp9OorWCMjG3JphWhRJNE3QEm5k5e+3M2/vtlP22A/Hr+yHxP6tzfveu0lcPgnOLEHCrPcQyYUHTs5by+q/QTKcPeZX8P1tQaSEh7NE5Ft2W7PZlDMIB666CF6RvT0yufTWmNPP0LZnlTsBw5QfuAA5Wnm2H7kiKc/fzNWhREWhrVNGywREVgqxhERGAEBOE6cwHHs2Mnh+HFwOKp+nA6xBPTsZSbx3ubYr0sXlNVqxnL4MKU7dlCalETJjiRKk5JwFRR49vc75xzCJ00i/DcTsXXs6JVrUFn+qtUceeABrFFRdJ4/D/8ePbx+DiHOBkn0jbAjPY8HPkog6Ug+4/q044mr+hEbXo8HtGWFUFZgVusYFjCsJwdlAcMAlwvK8qA4G4qOQ/GJqsPxVFypa/g0OJB/REdTgOZ3facxY8AsgmxBTfaZXeXl2A8dMpN++hGcubk4c3LMcW4Ojpxcczo7G11WhiUiAmt09MkhJqbKtP9552IJC2tQDNrlwn7oEKU7d2Lr2JGAuLgmr1oqSUzk0MyZ6JJSOr74IiEjf9Wk5xOiKUiibySH08Xb3+7nH2t3YzUM/nJZL6Zd2BXDOAsP7/KPwJZ3yf15IS/5l/NRaAjtLME8cMH9jOs5udkfIGqns0HVOi2d/ehRDs2YSdnu3bR76EHa3nhjc4ckRINIoj9DB08U89AniXydepxBXdrw9OQ4erdv2J1qozntkPw52356jSfLD5Li78evLG24t+M4eoZ1NZ8HBISBf7h77J63BTXLbwJaM2dhEUf+/GcK168nYto02s15wOebqQrfIYneC7TWLP85nSc/30luiZ0J/dtz15ge9O1wlhI+4MhI4r1vHuO1gl0UGYrhxSXcnFfAsNJSTkvpfqEQ0RXadK1+7Ne6m1o2Fe10kvXc82S/+y7Bw4cRPOJXKJsN5edX7bhKYVrb/xmlQHHym1hFU09U1XWVBxTKUGZ1nzLMJqKGAYalynLPtGG411ez3GIxv4FZLOY2FrMasbm/GQrvkUTvRTlF5bz1zX7e/S6NgjIH4/q0456x5xHfqc1ZiyGvJIcPdi1hye5lHC/LoWdwJ26OHcmEkHOxlRdCWT7kH4XcA5BzwBzbT+nXxxqAJ8lUN7ZYzW8FtiDwCwJbsHscZBYSVn93KyJnza2LtNNsfaRd7pYurqqDqniGccqzjIp57TS/0TjL3cMp0y7nyXbxnoRZMe3+nBXndVXE4h67XOa7hruNhB6/hnNGVin8ct7/D5nPPIMuKzsb/6TNx530KwoIpVTVQsBiqVQQGScLIUOhKq55pcKj1vGphZHFMI9pcRdcqmJ9NdOeAhBQqmosnmWVC05zUMYpy077PUVt++MpyD371KqO9bXtX22+rbosoF8/QkaOrOXwkui9Lq/YzsLv0nj72/3kldgZ1TOae8aex+Cubc9aDOXOclbuX8m7Se+yJ3cPMYEx3NDnBq7tdS1hfpW+aWhtPvTNPQA5aea4JBfQJ//AKloCVYyddrOVkb0IyovNgqK8yD0uBkfp6Ym5yrzFncgt7v9QRqX/WO5kXJGAqyskXHZzf6sfWCoGW9Xpyi2YKgoTqDptWCr9p3bHUhFTcTakfW1+Jouf+RuJHpeaiT/yPLTTiS4rw1Vejrbb0eV2tL3cPbajy8tP/0ep7v+y5vRr7Z7WLl312mvtbgLKyUJKa7Mba63RTie4tKew0k6XuW/FtHaZ27q30U4nOF1ol3vsNAvnU5eb+2lwOtHavb/L6T7myRjNOPA0VwVtblOxbcXY6TS3rRi7B33auOL8+mQMFesqmsS6nJWux6nX6fRBc8q2LlfVZRXXEjzTlf9NWqqIG35L+7/+tcb1kuibUEGpnX9/f4B/fb2f7KJyhp8byawx5zGse+TZeWiLWa303ZHvWJi0kO+Pfk+QNYire1zNjX1upHNo57MSQ6vlKIMD38GeLyF1DRzfbS5v0xXOHWP+PsIWYH6TsbrHlec9BZDN/EX1qdMVBZ6ngKk8LdUmLZE+NfHXpxCoY319smm1fw2V/0bcVXA17i+JvukVlztY+sNB3ti4j2MFZXRuG8g1gzpxzaBOdG7bdE0iT5WcncyipEV8kfYFTpeTS7pcwu/6/o5BMYN8qj5Wa02ps5RAq5f7JMo5AHvWQuqXZgFQlk/9/ps2hqr6DaPytw6lKi2v+MZUebriG1PFMkv131pqHNTp06ga5itiraGar9axe98zvlSVnm00Npa6PqdxSoF86rfS2q5xfdd5Cnt1+nmqi6FKPLX/cFIS/VlUaneyakcGH2w5xHd7T6A1DOseyZTBnZgQ154gv7PTiiOrOIv3k99n2e5l5JXl0TeyL7/r+zvGdx2PzctdN5xtRfYipq+dTsKxBKICo+gU0olOoe4h5OQ4Oij6jH5VDJh3ao4ycJS4q7IqDY6SSs8O7GZ1k9NhLquYdjlOPhtwVXpmceozA88yXXW5y3ly3yrTjqrrqxzz1OOf+nyk8rzz9Oov7a4OqlR1VKVar84xeApHz48DzyTZN+TctfwY0RdUW5iYBYf6v4OS6JvD4Zxilv+czodbDnMwu5hgPwtXxMdyzaBODOnWFstZqNopcZTw373/ZfGuxezP209MYAy/7fNbrulxDREBEU1+fm+zu+zc9dVd/HD0B27qexN55XkcLjjM4YLDZBRnVHnZepA1iD6RfegX2Y/+Uf3pF9mPzqGdfeqbjaiFrq4gOKWwq7zMVblArjw+teB1VVpeTSF+6rpTC2ytqz+P1tUsc9V8fs86c15d8YIk+uakteantBw+2HyIzxOPUlzuJDLYj7F9YhjXpx0je0QT6Ne0Pz5yaRffpn/Lv3f+m01HN2FRFga1G8TYLmMZ03kMHUI6NOn5vUFrzcPfPsyKvSv42/C/cXWPq6ustzvtHC06aib+wsPsyd1D0okkkk8kU+4yH5yG+oXSL7If/SL7MTR2KMM7DG+OjyKE10nVTQtSVObgq+QsvtyZybqULApKHfhbDUb2iOLSvu24pHc7okObtifFPTl7WLl/JesOrWNP7h4A+rTtw5guY7ik8yX0jOjZIu96X9ryEm/teIu7BtzFHeffUe/97C47e3P3knQ8iR0ndpB0PInUnFQc2sHckXO5ovsVTRi1EGeHJPoWqtzh4sf92Xy5K5O1OzNJzy1BKRjYuQ2X9I5hdK8Y+saGNWnrnQP5B1h3cB3/O/Q/tmVtQ6PpGNKRiztdTHx0PPFR8S2iumPprqU88+MzTO05lYcveviM4yl1lHLH2jvYlb2L9ye+T/fw7l6KVIjmIYm+FdBas+toAWt3ZvLlrkwS0/MAiA71Z1TPaMb0iuFXPaIID2y6B6nHS46z4dAGvjr4FZszN1PiKAEg3D+c/lH9iY+Kp39Uf+Ki4s5q/f6atDX8ecOfGd15NC+OfhGL4Z1qrsyiTKZ+NpW2AW1ZesVS77fgEeIskkTfCh0rKGPj7mOsS8ni69Tj5JXYsRiKwV0iGNUrmlE9o5v0bt/hcrA3dy+JxxM9w97cvZ6HnR1DOtI9vDvdwrtxTvg5dAszx5EBkV69+9+csZk71t5B38i+vPnrNwmwBnjt2ADfpX/HnV/eyaTzJvHEiCe8emwhziZJ9K2cw+li26Fc1qccY/3uLHakm29Pigz241c9ohjZI5qRPaJoF+bdJHiqInsRO0/sJPF4IjtP7CQtL40D+QcodZZ6tgm1hdItvBvdwrrRMbQjHYI70CGkAx2CO9A+uH2DmnbuydnDTatuIiowikWXLarxjVtn6pWtr7AgYQFPjniSSedNapJzCNHUJNH7mKyCUr7dc5yNu4/zdeoxjheaLUp6tQtlZI8oRvaMZmi3tk3ekgfM1jwZRRmk5aWxP38/+/P2k5ZvFgBZxVlVmjsqFNFB0XQI7kBsSCxRgVG0DWhLhH8EbQLaeKYjAiIothfzuy9+h0u7WHz54iZtFeR0OfnD2j+QeCyR9654j/MizmuycwnRVJok0Sul3gYmAlla6/61bHcBsAm4Xmv9YV0nlkTfMC6XJjmjgK9Tj/F16nF+TMum3OHCaij6dQzngq4RDOnWliHdIogKObvvRbW77GQWZXKk8AhHio6Y40rT2aXZnucA1QmxhbDwsoX0aturyWM9VnyMa/97LeH+4bx3xXtN+oIXIZpCUyX6i4FCYFFNiV4pZQHWAqXA25Lom15JuZMf07L5cf8Jftqfw7bDuZQ7zLvq7tHBXNDVTPoDu7ShW2QwVssZ/nL0DJU6SskpzSG7LJvc0lyyS7PJKc0hrzyPS7pcQr/Ifmctlu+Pfs/0NdOZ2H0iT/3qqWZvaSREQzQm0df5O32t9UalVLc6Nrsb+Ai4oCEnF40X6GdhVE/zYS1AmcPJjvQ8fkrLYXNaNquSMvjP5kMA+FkNesSE0Lt9GH1iQ+nVPpTe7cOavB1/ZQHWAGJDYokNiT1r56zJRbEXMWPADF7f9jpD2g9hco/JzR2SEE3qjDtkUUp1BK4GxlBHoldKTQemA3Tp0uVMTy0q8bdaGNy1rdl98qhzcbk0e44VsiM9j+SMAnYdzWdj6jE++vmwZ5+oED96tw+jb4cw+rmHc6JCzkqXDc1tetx0fs78mad/eJp+kf3OSrWREM2lXg9j3Xf0n1VXdaOU+gD4u9b6e6XUQvd2UnXTQp0oLCMlo4BdGQWkZOSz82g+uzMKKXea1T4BNoPe7SsSfzh9O4TRu30oATbfeW9shRMlJ7j2v9cSZAviroF30bdtXzqFdjrzDtKEaEJN1uqmjkS/n5Nd1kUBxcB0rfUntR1TEn3LYXe62JNVSNKRfJKO5JF0JJ9dR/IpKHMAYDEU50YH0zf2ZPLvGxtGRLBfM0d+5jZnbGbWV7Modphv5gq2BdO7bW/6tO1Dn8g+9Gnbh3PCz8FqyDtlRcvQLIn+lO0WInf0PkFrzaHsEpKO5LHzaD5JR/LZeSSfjPyTbek7hAfQt0MYPduF0rNdKD3ahXBudEiru/svd5azJ3cPydnJ7Dyxk+TsZFKyUzy/G/Az/IgKjCLcP/zk4HdyOswvjFC/UIJsQQTbggm2BhNsC/bMSyEhvKmpWt28B4zGvFvPBB4FbABa6/mnbLsQSfQ+7XhhGTuP5FdK/nmknSjG6TL/jgwFXSOD6RETQo92IfRsF8q50SGcExVMsH/rSXhOl5O0/DR2Ze8iJTuF7NJscstyySvLI68sj/zyfPLK8nBqZ53H8rf4E2QNItAaSIA1oMq4YgiwBOBv9SfAEoCfxa/KvL/FH3+rvzm2+ONn8asyrpj2M/zws/hJ1ZOPkx9MiWZR7nCx/3gRuzMLSM0qJDWzgN2ZBVUKAIB2Yf50jwrhnOhgukcF0z06mHOiQugcEdjszT8bw6VdFNmLyC3Lpche5BmK7cUn5x3mfImjpMpQ6iitOnaWUuoopcxZVq/CozZWZcVmsVVJ/n4WP2yGDT/Dz1xn+GG1WM15w+ZZbzNs2Cy2k9OV5q2G1bPMapjnsClzfcU5rYb15LSymvPu/SpPWwwLVmX1Wn9GvyRN0rxSiLr4WQ16tTebbVZWUQDsO1bIvuNF7DtWxP7jhaxMPEpusd2znc2i6NI2iO7RIXT3FAIhdI8Kpm2wX4tt524og1C/UEL9QuveuAHsLjtljjJKnWbir5gud5ZT7iynzFnmGVeeLneVY3faKXed3M7usnv2s7vMdQ6ng3JXOUX2InMf9zZ2lx2Hy3Fy7LTj0A6vfrZTKZSnAPAUAsqC1bB6xhVDdcstyoLFsHj2sxiWKttVmT51XaXCpvJyQxknpw3D3KbS/hXrDWV4zlGxXcUyQxlVtq9uv8pjQxlN+ncuiV40mZoKAICconL2HS9k77Gik4XBsSI2pBzztAACCA+00S0qmM4RgXRuG0TniCA6tw2kc0QQHdoE4mdtfd8E6mIzbNj8bIQQ0tyh4NIuHC4H5c5yHC4HDm0WAHaXvUrBYHfZzYKhYptK6ysXHhWDUztPW1ax3DOvHThdJ+ft2o7L5cKhzflSZ6lnfcV+Tu00l7n3rTxfsb5ydx0tSUXin9prKnOGzvHqsSXRi2YREezH4GB3u/9KnC7N4ZxizzeAfccKOZhdzI70PFYnZWB3nqwKMhS0DwugU0QQ7cIDaB/mT7uwANqHB5jjsABiwvzxt0r1QGMZyvBU/fgKrbVZAFRTKHgKA3eB4iks3NtWTFcUgC7t8qxzaXOfimUu7fLsU3m7yvPVbRsfHe/1zyyJXrQoFkPRNTKYrpHBjDnlN0xOlyYjv5RD2cXmkFPC4exiDueWkHA4lzV5pZQ5Tr9baxvsR0yoP9Gh/sSEmsk/ptK0WVgEttgqIuFdSinz+QFW+IXcA0iiF62GxVB0bBNIxzaBXNQ98rT1WmvySxxk5JeSkV9KZl6pZzorv4xjBaXsySrkWEEZDlfVRggRQTYGdYlgUNcIhnSNIL5Tm7PSK6gQZ4MkeuEzlFKEB9kID7JV+1yggsulySkuJ6ugjKyCMtJzSth2KIctB3L4KjkLwOwVtEMYg7pGMKhLBB3aBBIeaCUs0EZ4oE2qg0SrIs0rhagkp6icre6kvzkth+2Hcym1n14d5G81CA+0eRJ/iL/VMwT7Wwnxt5jjAHNZkJ+VQJuFQD+DAJvFPW2OA2wW/K1N2+pC+A5pXinEGYoI9uOS3u24pHc7wOweIiWjgOOFZeSV2MkvsZNf6vBM57mHnOJyDuUUU1jqoKjMQVF5w9rCK2UWHhVJP8BmIcBqwd9meMb+VgM/q4G/1YKfxcDfZlQaW/CzGtgsyrOdOW9u42c1xzb3MptFmfOeZea81WJgNczppnxZvTi7JNELUQubxaB/x/AG7+dyaYrtTorKHBSUOigpd1Jidw/lTkorTZfYzfkyh8sc212UOZyUVhoXljnILnJR7nBR7nRRZq8YOyl3uqq0RvIWQ+EuFAysFoXVMAsEq0VhM05dZhYQlZdZDHO5zVBYKi9zz1urzJ9cbjEwxwos7uNalLnNaYNSWCzm2GooDOOUsTJjsqiqyyr290wrhWHgWVax3FD4xDctSfRCNAHDUJ6qnHZhTX8+p0tjd7ooc5iFgd15slAod5jL7c6TQ7lDV513ahyeeXOdw6mxu1zY3ds6XOY2DtfJ9Q6XuX3FuNTuwuF0uLc193dWTDtduLTG4dI4ne6xy9zX1Tw1yPWiFJ6CwqLM5G94CgSzcFCq+nWGUub+7mWq0vKKQuTkvLltdeOTx2pcoSOJXggfYN6hWlpdh3IVXC4z8XsKglMGh7vAcNa6jfYcx6lPTnuWVQzudZ6xS+PUnFzmXu7SeM7n0hqniyrrzGUn17tcePbX7vUu93ErHwNO7u/SZmuxyvtXHFNz+jbORv7WSxK9EKLZGYbCT54J1Iua3fB9fO/340IIIaqQRC+EED5OEr0QQvg4SfRCCOHjJNELIYSPk0QvhBA+ThK9EEL4OEn0Qgjh4yTRCyGEj5NEL4QQPk4SvRBC+DhJ9EII4eMk0QshhI+TRC+EED5OEr0QQvg4SfRCCOHjJNELIYSPk0QvhBA+ThK9EEL4OEn0Qgjh4yTRCyGEj5NEL4QQPq7ORK+UelsplaWU2lHD+huVUglKqUSl1HdKqfO9H6YQQojGqs8d/ULgslrW7wdGaa3jgCeABV6ISwghhJdY69pAa71RKdWtlvXfVZr9HujkhbiEEEJ4ibfr6G8DvqhppVJqulJqs1Jq87Fjx7x8aiGEENXxWqJXSo3BTPQP1LSN1nqB1nqI1npIdHS0t04thBCiFnVW3dSHUioe+BcwQWt9whvHFEII4R1nfEevlOoCLAd+p7XefeYhCSGE8KY67+iVUu8Bo4EopdRh4FHABqC1ng/8FYgEXldKATi01kOaKmAhhBANU59WN7+tY/3twO1ei0gIIYRXyS9jhRDCx0miF0IIHyeJXgghfJwkeiGE8HGS6IUQwsdJohdCCB8niV4IIXycJHohhPBxkuiFEMLHSaIXQggfJ4leCCF8nCR6IYTwcZLohRDCx0miF0IIHyeJXgghfJwkeiGE8HGS6IUQwsdJohdCCB8niV4IIXycJHohhPBxkuiFEMLHSaIXQggfJ4leCCF8nCR6IYTwcZLohRDCx0miF0IIHyeJXgghfJwkeiGE8HGS6IUQwsdJohdCCB8niV4IIXycJHohhPBxkuiFEMLHSaIXQggfJ4leCCF8XJ2JXin1tlIqSym1o4b1Sin1slJqj1IqQSk1yPthCiGEaKz63NEvBC6rZf0EoId7mA7MO/OwhBBCeEudiV5rvRHIrmWTScAibfoeaKOUivVWgEIIIc6M1QvH6AgcqjR/2L3s6KkbKqWmY971AxQqpVJqOW4UcNwL8TUlidE7JEbvaOkxtvT4oHXE2KuhO3gj0deb1noBsKA+2yqlNmuthzRxSGdEYvQOidE7WnqMLT0+aD0xNnQfb7S6SQc6V5rv5F4mhBCiBfBGol8B3ORufXMRkKe1Pq3aRgghRPOos+pGKfUeMBqIUkodBh4FbABa6/nASuByYA9QDNzqpdjqVcXTzCRG75AYvaOlx9jS4wMfjVFprZsiECGEEC2E/DJWCCF8nCR6IYTwcS0y0SulLlNKpbi7VZjT3PFURymVppRKVEpta0xzp6ZQXXcVSqm2Sqm1SqlU9ziiBcb4mFIq3X0ttymlLm/G+DorpdYppXYqpZKUUn90L28x17GWGFvSdQxQSv2olNrujvFx9/JzlFI/uP9v/0cp5dcCY1yolNpf6ToOaK4Y3fFYlFJblVKfuecbfg211i1qACzAXqA74AdsB/o2d1zVxJkGRDV3HKfEdDEwCNhRadlzwBz39Bzg2RYY42PAn5v7+rljiQUGuadDgd1A35Z0HWuJsSVdRwWEuKdtwA/ARcAy4Hr38vnAjBYY40JgSnNfw0px/glYCnzmnm/wNWyJd/RDgT1a631a63LgfcxuFkQddPXdVUwC3nVPvwtcdTZjOlUNMbYYWuujWuuf3dMFwC7MX3q3mOtYS4wthjYVumdt7kEDlwAfupc393WsKcYWQynVCbgC+Jd7XtGIa9gSE31NXSq0NBpYo5Ta4u7aoaVqp0/+riEDaNecwdTiLnfvp283d/VSBaVUN2Ag5p1ei7yOp8QILeg6uqsctgFZwFrMb+q5WmuHe5Nm/799aoxa64rr+JT7Or6olPJvvgh5CfgL4HLPR9KIa9gSE31r8Sut9SDM3jtnKaUubu6A6qLN73ot6o7FbR5wLjAAs4+kvzdrNIBSKgT4CJittc6vvK6lXMdqYmxR11Fr7dRaD8D8tfxQoHdzxlOdU2NUSvUH/g8z1guAtsADzRGbUmoikKW13nKmx2qJib5VdKmgtU53j7OAjzH/kFuizIreRN3jrGaO5zRa60z3fzgX8CbNfC2VUjbMBLpEa73cvbhFXcfqYmxp17GC1joXWAcMw+zdtuKHmi3m/3alGC9zV41prXUZ8A7Ndx1HAFcqpdIwq7AvAf5JI65hS0z0PwE93E+W/YDrMbtZaDGUUsFKqdCKaeDXQLUvZmkBVgA3u6dvBj5txliqpap2a301zXgt3XWgbwG7tNb/qLSqxVzHmmJsYdcxWinVxj0dCFyK+SxhHTDFvVlzX8fqYkyuVKArzPrvZrmOWuv/01p30lp3w8yD/9Na30hjrmFzP1Gu4Snz5ZgtCfYCDzV3PNXE1x2zNdB2IKmlxAi8h/mV3Y5Zd3cbZp3eV0Aq8CXQtgXG+G8gEUjATKixzRjfrzCrZRKAbe7h8pZ0HWuJsSVdx3hgqzuWHcBf3cu7Az9idpnyAeDfAmP8n/s67gAW426Z05wDZjc0Fa1uGnwNpQsEIYTwcS2x6kYIIYQXSaIXQggfJ4leCCF8nCR6IYTwcZLohRDCx0miF0IIHyeJXgghfNz/A3ZWAQrnFwWPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(train_loaded, label=\"train_loss_month\")\n",
    "line2, = ax.plot(test_loaded, label=\"test_loss_month\")\n",
    "line3, = ax.plot(train_loaded_2, label=\"train_loss_add_block\")\n",
    "line4, = ax.plot(test_loaded_2, label=\"test_loss_add_block\")\n",
    "ax.set_ylim([1, 2])\n",
    "ax.set_xlim([-1, 40])\n",
    "# Create a legend for the first line.\n",
    "first_legend = ax.legend(loc='upper right')\n",
    "# Add the legend manually to the Axes.\n",
    "ax.add_artist(first_legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f2c975-5cf3-41e8-92ec-45b3a24347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name = \"train_loss.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(train_loss, open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = \"test_loss.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(test_loss, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacde853-3432-4377-b540-0e48c5f6e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_4=train_loaded+train_loss\n",
    "test_4=test_loaded+test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ac7a28-156c-49e0-8f46-862a8fecede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"train_loss_4.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(train_4, open_file)\n",
    "open_file.close()\n",
    "file_name = \"test_loss_4.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(test_4, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5d5b1ed-5e32-44fb-a245-0e5832555663",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7],[8,9,10,11,12,13,14]])\n",
    "x = x.unsqueeze(2).unsqueeze(3)\n",
    "x = x.repeat(1,1,3,3)\n",
    "y = x\n",
    "z = torch.cat((x,y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "991123d9-6950-4c0c-909a-f9613c865acd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1]],\n",
       "\n",
       "         [[ 2]],\n",
       "\n",
       "         [[ 3]],\n",
       "\n",
       "         [[ 4]],\n",
       "\n",
       "         [[ 5]],\n",
       "\n",
       "         [[ 6]],\n",
       "\n",
       "         [[ 7]]],\n",
       "\n",
       "\n",
       "        [[[ 8]],\n",
       "\n",
       "         [[ 9]],\n",
       "\n",
       "         [[10]],\n",
       "\n",
       "         [[11]],\n",
       "\n",
       "         [[12]],\n",
       "\n",
       "         [[13]],\n",
       "\n",
       "         [[14]]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1],[2],[3],[4],[5],[6],[7]],[[8],[9],[10],[11],[12],[13],[14]]])\n",
    "x = torch.unsqueeze(x, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcee774-091a-4b94-9ebd-b02f29fa8712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 3, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.zeros((3,3,3,3))\n",
    "x[:,np.array([0,2]),:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL3 (my_npl_clone)",
   "language": "python",
   "name": "npl3-my_npl_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
