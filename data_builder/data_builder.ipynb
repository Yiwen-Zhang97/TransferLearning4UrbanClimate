{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4edce6-95fa-4cc5-b8d2-51dd099cfa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tfrecords2numpy import TFRecordsParser\n",
    "from tfrecords2numpy import TFRecordsElevation\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import webdataset as wds\n",
    "import xarray as xr\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc18a75b-4356-4350-add8-9b16e13f9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FORCING = \"/glade/work/yiwenz/TransferLearning/forcing_daily.pkl\"\n",
    "PATH_TO_DATA = \"/glade/scratch/yiwenz/Data_TFRecord_Daily/\"\n",
    "PATH_TO_STORE = \"/glade/scratch/priyamm/transfer_learning_data/phoenix_data.h5\"\n",
    "PATH_TO_LATLONG = \"/glade/u/home/yiwenz/TransferLearning/modis_lon_lat.csv\"\n",
    "PATH_TO_ELEVATIONS = \"/glade/work/yiwenz/AWS3D30_cropped.tfrecord\"\n",
    "\n",
    "channel_choices = ['Red', 'Green', 'Blue', \"NIR\", \"SWIR1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14f4d444-1a44-4b30-ba08-a1017d6b5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_distance_mean():\n",
    "    latlong = pd.read_csv(PATH_TO_LATLONG, usecols=[\"lon\", \"lat\"])\n",
    "    array = latlong.values\n",
    "    dist = euclidean_distances(array, array)\n",
    "    weights_idx_dict = {}\n",
    "    for idx in range(len(dist)):\n",
    "        sort_idx = dist[idx, :].argsort()[1:8]\n",
    "        closest = dist[idx, sort_idx]**-1\n",
    "        closest = closest / closest.sum()\n",
    "        weights_idx_dict[idx] = {\"weights\":closest, \"idx\":sort_idx}\n",
    "\n",
    "    return weights_idx_dict\n",
    "\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extract all data from all TFRecords files and stores as pickled tuples in the format (image, label)\n",
    "    :return:\n",
    "    Each Sample is of Array Size 8721. For a single sample arr, this is how we can access our data:\n",
    "    arr[:7] -> 7 Forcing Variables\n",
    "    arr[7] -> Month Indicator of Data\n",
    "    arr[8] -> LST Target\n",
    "    arr[9:].reshape(8,33,33) -> Sattelite Image Tensor \n",
    "    \"\"\"\n",
    "    tot_samples = 0\n",
    "    first_batch = True\n",
    "    with h5py.File(path, \"a\") as hp5:\n",
    "        center_weight = 1\n",
    "        outside_weight = 1 - center_weight\n",
    "        weights_idx_dict = inverse_distance_mean()\n",
    "        elevations_dict = TFRecordsElevation(filepath=PATH_TO_ELEVATIONS).tfrecrods2numpy()\n",
    "        with open(PATH_TO_FORCING, 'rb') as f:\n",
    "            forcing_data = pickle.load(f)\n",
    "\n",
    "        avail_dates = list(forcing_data.keys())\n",
    "\n",
    "        for root, dirs, files in os.walk(PATH_TO_DATA):\n",
    "            random.shuffle(files)\n",
    "            for file in tqdm(files, desc=\"Total Progress\"):\n",
    "                samples = []\n",
    "                path_to_tf = os.path.join(PATH_TO_DATA, file)\n",
    "                file_date = file.split(\".\")[0]\n",
    "                records = TFRecordsParser(path_to_tf, channels=channel_choices).tfrecrods2numpy()\n",
    "\n",
    "                if file_date in avail_dates:\n",
    "                    month = int(file_date[2:4])\n",
    "                    for idx, (features, lst) in enumerate(records):\n",
    "\n",
    "                        if (lst is not False) and ((features!=-9999).all() == True):\n",
    "                            if outside_weight != 0:\n",
    "                                weights = np.array(weights_idx_dict[idx][\"weights\"]) * outside_weight\n",
    "                                min_idx = weights_idx_dict[idx][\"idx\"]\n",
    "                                surround_features = np.array([records[i][0] for i in min_idx])\n",
    "                                weighted_avg = np.average(surround_features, weights=weights, axis=0)\n",
    "                                features = np.average([features, weighted_avg], weights=[center_weight, outside_weight], axis=0)\n",
    "                            NIR_dn = (features[3]+0.2)/2.75e-05\n",
    "                            SWIR1_dn = (features[4]+0.2)/2.75e-05\n",
    "                            RED_dn = (features[0]+0.2)/2.75e-05\n",
    "\n",
    "                            features_ndbi = ((SWIR1_dn-NIR_dn)/(SWIR1_dn+NIR_dn)).reshape(-1,33,33)\n",
    "                            features_ndvi = ((NIR_dn-RED_dn)/(NIR_dn+RED_dn)).reshape(-1,33,33)\n",
    "                            features = np.concatenate([features,features_ndbi,features_ndvi],axis=0)\n",
    "                            elevations = elevations_dict[idx].reshape(-1,33,33)\n",
    "                            features = np.vstack([features, elevations])\n",
    "                            forcing = forcing_data[file_date][idx]\n",
    "\n",
    "                            sample_image = features.flatten()\n",
    "                            month = np.array(month).reshape(-1,)\n",
    "                            lst = np.array(lst).reshape(-1,)\n",
    "                    \n",
    "                            ex_array = np.concatenate((forcing, month, lst, sample_image))\n",
    "                            \n",
    "#                             VALIDATION CHECKS\n",
    "#                             reb_forcing = ex_array[:7]\n",
    "#                             reb_month = ex_array[7]\n",
    "#                             reb_lst = ex_array[8]\n",
    "#                             reb_image = ex_array[9:].reshape(8,33,33)\n",
    "                            \n",
    "#                             print(reb_forcing == forcing)\n",
    "#                             print(reb_month == month)\n",
    "#                             print(reb_lst == lst)\n",
    "#                             print((reb_image == features).all())\n",
    "                            \n",
    "                            samples.append(ex_array)\n",
    "                samples = np.array(samples)\n",
    "                new_samples_num = samples.shape[0]\n",
    "                if new_samples_num > 0:\n",
    "                    ## ONLY APPEND IF DATA EXISTS ##\n",
    "                    tot_samples += new_samples_num\n",
    "                    np.random.shuffle(samples)\n",
    "                    _, feature_length = samples.shape\n",
    "                    assert(feature_length == 8721)\n",
    "                    if first_batch:\n",
    "                        hdf5_dataset = hp5.create_dataset('phoenix', (new_samples_num, feature_length), maxshape=(None, feature_length), dtype='float32')\n",
    "                        hdf5_dataset[:] = samples\n",
    "                        first_batch = False\n",
    "\n",
    "                    hdf5_dataset.resize(tot_samples, axis=0)\n",
    "                    hdf5_dataset[-new_samples_num:] = samples\n",
    "    return \"DONE\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38248b50-4297-4ba2-9027-bf218575b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress: 100%|██████████| 6024/6024 [6:15:44<00:00,  3.74s/it]  \n"
     ]
    }
   ],
   "source": [
    "extract_data(PATH_TO_STORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c972d-3ece-44ea-b3cb-27de3ddb57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(PATH_TO_STORE, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b439d-b4c6-4bd0-aa3b-679a0a3283a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
