{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a9a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db198ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b87cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cb5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6838f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63e0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 15:06:56,847\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00000\n",
      "2022-04-17 15:06:58,713\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00001\n",
      "2022-04-17 15:06:58,720\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00002\n",
      "2022-04-17 15:06:58,725\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00003\n",
      "2022-04-17 15:06:58,734\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00004\n",
      "2022-04-17 15:06:58,784\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00005\n",
      "2022-04-17 15:06:58,793\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00006\n",
      "2022-04-17 15:06:58,800\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00007\n",
      "2022-04-17 15:06:58,808\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00008\n",
      "2022-04-17 15:06:58,816\tINFO trial_runner.py:803 -- starting train_cifar_eeb8a_00009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:06:56 (running for 00:00:00.21)\n",
      "Memory usage on this node: 6.6/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |\n",
      "| train_cifar_eeb8a_00001 | PENDING  |                     |            2 |   32 |    4 | 0.0240294   |\n",
      "| train_cifar_eeb8a_00002 | PENDING  |                     |            4 |   64 |   32 | 0.00156282  |\n",
      "| train_cifar_eeb8a_00003 | PENDING  |                     |            8 |    4 |    8 | 0.0923667   |\n",
      "| train_cifar_eeb8a_00004 | PENDING  |                     |            4 |   64 |    4 | 0.0155539   |\n",
      "| train_cifar_eeb8a_00005 | PENDING  |                     |            8 |  128 |  256 | 0.000899072 |\n",
      "| train_cifar_eeb8a_00006 | PENDING  |                     |            2 |    4 |    8 | 0.045046    |\n",
      "| train_cifar_eeb8a_00007 | PENDING  |                     |           16 |  256 |  128 | 0.000332309 |\n",
      "| train_cifar_eeb8a_00008 | PENDING  |                     |           16 |    4 |  128 | 0.000234467 |\n",
      "| train_cifar_eeb8a_00009 | PENDING  |                     |            4 |  256 |   32 | 0.000128987 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10453)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10453)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m Files already downloaded and verified\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:03 (running for 00:00:07.14)\n",
      "Memory usage on this node: 11.6/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:08 (running for 00:00:12.15)\n",
      "Memory usage on this node: 11.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1,  2000] loss: 2.305\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1,  2000] loss: 2.328\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1,  2000] loss: 2.347\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:13 (running for 00:00:17.16)\n",
      "Memory usage on this node: 11.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [1,  2000] loss: 2.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m [1,  2000] loss: 2.311\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m [1,  2000] loss: 2.307\n",
      "\u001b[2m\u001b[36m(func pid=10453)\u001b[0m [1,  2000] loss: 2.328\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [1,  2000] loss: 2.155\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:18 (running for 00:00:22.17)\n",
      "Memory usage on this node: 11.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [1,  2000] loss: 2.305\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1,  4000] loss: 1.152\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [1,  2000] loss: 2.291\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1,  4000] loss: 1.166\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1,  4000] loss: 1.177\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:23 (running for 00:00:27.18)\n",
      "Memory usage on this node: 11.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [1,  4000] loss: 0.891\n",
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m [1,  4000] loss: 1.156\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m [1,  4000] loss: 1.152\n",
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.1113\n",
      "  date: 2022-04-17_15-07-26\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3023063117980955\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 24.923730850219727\n",
      "  time_this_iter_s: 24.923730850219727\n",
      "  time_total_s: 24.923730850219727\n",
      "  timestamp: 1650226046\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10453)\u001b[0m [1,  4000] loss: 1.164\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.1491\n",
      "  date: 2022-04-17_15-07-28\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.2107159370422362\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 27.228038787841797\n",
      "  time_this_iter_s: 27.228038787841797\n",
      "  time_total_s: 27.228038787841797\n",
      "  timestamp: 1650226048\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [1,  4000] loss: 0.870\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1,  6000] loss: 0.766\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1,  6000] loss: 0.784\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1,  6000] loss: 0.775\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:33 (running for 00:00:37.07)\n",
      "Memory usage on this node: 11.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.256511124420166\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |         |            |                      |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |         |            |                      |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |         |            |                      |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |         |            |                      |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 2.21072 |     0.1491 |                    1 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.30231 |     0.1113 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [1,  6000] loss: 0.543\n",
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m [1,  6000] loss: 0.771\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m [1,  6000] loss: 0.767\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:38 (running for 00:00:42.08)\n",
      "Memory usage on this node: 11.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.256511124420166\n",
      "Resources requested: 10.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING  | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING  | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING  | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |         |            |                      |\n",
      "| train_cifar_eeb8a_00003 | RUNNING  | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   |         |            |                      |\n",
      "| train_cifar_eeb8a_00004 | RUNNING  | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |         |            |                      |\n",
      "| train_cifar_eeb8a_00005 | RUNNING  | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 |         |            |                      |\n",
      "| train_cifar_eeb8a_00006 | RUNNING  | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING  | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 2.21072 |     0.1491 |                    1 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING  | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.30231 |     0.1113 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING  | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00003:\n",
      "  accuracy: 0.0997\n",
      "  date: 2022-04-17_15-07-39\n",
      "  done: true\n",
      "  experiment_id: 5bf1e4efc72e4d71ae316e5c39264ad3\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.336529919052124\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10453\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 38.017127990722656\n",
      "  time_this_iter_s: 38.017127990722656\n",
      "  time_total_s: 38.017127990722656\n",
      "  timestamp: 1650226059\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00003\n",
      "  warmup_time: 0.002886533737182617\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1,  8000] loss: 0.573\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1,  8000] loss: 0.589\n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.413\n",
      "  date: 2022-04-17_15-07-41\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.6190190135478972\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 40.32693910598755\n",
      "  time_this_iter_s: 40.32693910598755\n",
      "  time_total_s: 40.32693910598755\n",
      "  timestamp: 1650226061\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [2,  2000] loss: 2.301\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1,  8000] loss: 0.581\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [1,  8000] loss: 0.391\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [2,  2000] loss: 2.099\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:46 (running for 00:00:50.09)\n",
      "Memory usage on this node: 11.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.256511124420166\n",
      "Resources requested: 9.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (9 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |         |            |                      |\n",
      "| train_cifar_eeb8a_00004 | RUNNING    | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |         |            |                      |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.61902 |     0.413  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 2.21072 |     0.1491 |                    1 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.30231 |     0.1113 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING    | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |         |            |                      |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m [1,  8000] loss: 0.578\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m [1,  8000] loss: 0.574\n",
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.16\n",
      "  date: 2022-04-17_15-07-48\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 2\n",
      "  loss: 2.2943727989196776\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 47.23774981498718\n",
      "  time_this_iter_s: 22.314018964767456\n",
      "  time_total_s: 47.23774981498718\n",
      "  timestamp: 1650226068\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1, 10000] loss: 0.450\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1, 10000] loss: 0.471\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1, 10000] loss: 0.466\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.3001\n",
      "  date: 2022-04-17_15-07-53\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.9199252038955688\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 51.692705154418945\n",
      "  time_this_iter_s: 24.46466636657715\n",
      "  time_total_s: 51.692705154418945\n",
      "  timestamp: 1650226073\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:53 (running for 00:00:56.51)\n",
      "Memory usage on this node: 11.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.107149001407623 | Iter 1.000: -2.256511124420166\n",
      "Resources requested: 9.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (9 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |         |            |                      |\n",
      "| train_cifar_eeb8a_00004 | RUNNING    | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |         |            |                      |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.61902 |     0.413  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.91993 |     0.3001 |                    2 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.29437 |     0.16   |                    2 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING    | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |         |            |                      |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [1, 10000] loss: 0.304\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [2,  2000] loss: 1.524\n",
      "\u001b[2m\u001b[36m(func pid=10454)\u001b[0m [1, 10000] loss: 0.463\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:07:58 (running for 00:01:01.53)\n",
      "Memory usage on this node: 11.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.107149001407623 | Iter 1.000: -2.256511124420166\n",
      "Resources requested: 9.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (9 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  |         |            |                      |\n",
      "| train_cifar_eeb8a_00004 | RUNNING    | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   |         |            |                      |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.61902 |     0.413  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.91993 |     0.3001 |                    2 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.29437 |     0.16   |                    2 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING    | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |         |            |                      |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10465)\u001b[0m [1, 10000] loss: 0.457\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1, 12000] loss: 0.393\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1, 12000] loss: 0.358\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1, 12000] loss: 0.388\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.4436\n",
      "  date: 2022-04-17_15-08-02\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.5751665456056594\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 61.71883726119995\n",
      "  time_this_iter_s: 61.71883726119995\n",
      "  time_total_s: 61.71883726119995\n",
      "  timestamp: 1650226082\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [3,  2000] loss: 2.262\n",
      "Result for train_cifar_eeb8a_00004:\n",
      "  accuracy: 0.101\n",
      "  date: 2022-04-17_15-08-05\n",
      "  done: true\n",
      "  experiment_id: 037e8e5d9a2141eebd9785cd99cfb3fb\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.304492740535736\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10454\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 64.71348524093628\n",
      "  time_this_iter_s: 64.71348524093628\n",
      "  time_total_s: 64.71348524093628\n",
      "  timestamp: 1650226085\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00004\n",
      "  warmup_time: 0.002711772918701172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:05 (running for 00:01:09.17)\n",
      "Memory usage on this node: 11.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.107149001407623 | Iter 1.000: -2.256511124420166\n",
      "Resources requested: 9.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (9 RUNNING, 1 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | RUNNING    | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.61902 |     0.413  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.91993 |     0.3001 |                    2 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.29437 |     0.16   |                    2 |\n",
      "| train_cifar_eeb8a_00009 | RUNNING    | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 |         |            |                      |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [2,  4000] loss: 0.728\n",
      "Result for train_cifar_eeb8a_00009:\n",
      "  accuracy: 0.1508\n",
      "  date: 2022-04-17_15-08-07\n",
      "  done: true\n",
      "  experiment_id: 895e08448d7144e9ae228b01aa7588ed\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.2737658075332643\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10465\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 66.11369943618774\n",
      "  time_this_iter_s: 66.11369943618774\n",
      "  time_total_s: 66.11369943618774\n",
      "  timestamp: 1650226087\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00009\n",
      "  warmup_time: 0.0029420852661132812\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1, 14000] loss: 0.336\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1, 14000] loss: 0.297\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [3,  2000] loss: 1.825\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1, 14000] loss: 0.332\n",
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.187\n",
      "  date: 2022-04-17_15-08-09\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 3\n",
      "  loss: 2.1696958528518677\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 68.25019407272339\n",
      "  time_this_iter_s: 21.012444257736206\n",
      "  time_total_s: 68.25019407272339\n",
      "  timestamp: 1650226089\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [2,  2000] loss: 1.436\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:14 (running for 00:01:18.18)\n",
      "Memory usage on this node: 10.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.107149001407623 | Iter 1.000: -2.2737658075332643\n",
      "Resources requested: 7.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.61902 |     0.413  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.91993 |     0.3001 |                    2 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.1697  |     0.187  |                    3 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1, 16000] loss: 0.294\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.3825\n",
      "  date: 2022-04-17_15-08-16\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 3\n",
      "  loss: 1.7026560928344727\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 75.31707215309143\n",
      "  time_this_iter_s: 23.624366998672485\n",
      "  time_total_s: 75.31707215309143\n",
      "  timestamp: 1650226096\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.4832\n",
      "  date: 2022-04-17_15-08-17\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.4147882981777191\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 75.64254021644592\n",
      "  time_this_iter_s: 35.315601110458374\n",
      "  time_total_s: 75.64254021644592\n",
      "  timestamp: 1650226097\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1, 16000] loss: 0.253\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1, 16000] loss: 0.291\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:22 (running for 00:01:25.41)\n",
      "Memory usage on this node: 10.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.9199252038955688 | Iter 1.000: -2.2737658075332643\n",
      "Resources requested: 7.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.41479 |     0.4832 |                    2 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.70266 |     0.3825 |                    3 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.1697  |     0.187  |                    3 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [2,  4000] loss: 0.708\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [4,  2000] loss: 2.086\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1, 18000] loss: 0.261\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1, 18000] loss: 0.223\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:27 (running for 00:01:30.42)\n",
      "Memory usage on this node: 10.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.9199252038955688 | Iter 1.000: -2.2737658075332643\n",
      "Resources requested: 7.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.41479 |     0.4832 |                    2 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.70266 |     0.3825 |                    3 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.1697  |     0.187  |                    3 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1, 18000] loss: 0.259\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [3,  2000] loss: 1.343\n",
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.2586\n",
      "  date: 2022-04-17_15-08-30\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 4\n",
      "  loss: 2.006841798591614\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 89.31097316741943\n",
      "  time_this_iter_s: 21.060779094696045\n",
      "  time_total_s: 89.31097316741943\n",
      "  timestamp: 1650226110\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [2,  6000] loss: 0.470\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [4,  2000] loss: 1.660\n",
      "\u001b[2m\u001b[36m(func pid=10459)\u001b[0m [1, 20000] loss: 0.235\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:35 (running for 00:01:39.23)\n",
      "Memory usage on this node: 10.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -2.006841798591614 | Iter 2.000: -1.9199252038955688 | Iter 1.000: -2.2737658075332643\n",
      "Resources requested: 7.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.41479 |     0.4832 |                    2 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.70266 |     0.3825 |                    3 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.00684 |     0.2586 |                    4 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [1, 20000] loss: 0.192\n",
      "\u001b[2m\u001b[36m(func pid=10449)\u001b[0m [1, 20000] loss: 0.233\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.4165\n",
      "  date: 2022-04-17_15-08-40\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.5999722444534301\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 98.9944269657135\n",
      "  time_this_iter_s: 23.67735481262207\n",
      "  time_total_s: 98.9944269657135\n",
      "  timestamp: 1650226120\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [2,  8000] loss: 0.343\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [3,  4000] loss: 0.664\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:45 (running for 00:01:48.82)\n",
      "Memory usage on this node: 10.1/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.9199252038955688 | Iter 1.000: -2.2737658075332643\n",
      "Resources requested: 7.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 |         |            |                      |\n",
      "| train_cifar_eeb8a_00001 | RUNNING    | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   |         |            |                      |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.41479 |     0.4832 |                    2 |\n",
      "| train_cifar_eeb8a_00006 | RUNNING    | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    |         |            |                      |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.59997 |     0.4165 |                    4 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.00684 |     0.2586 |                    4 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [5,  2000] loss: 1.933\n",
      "Result for train_cifar_eeb8a_00006:\n",
      "  accuracy: 0.0977\n",
      "  date: 2022-04-17_15-08-47\n",
      "  done: true\n",
      "  experiment_id: eb9170f8f7a240e79dbca7d85e33c010\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.411849731063843\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10459\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 105.52285099029541\n",
      "  time_this_iter_s: 105.52285099029541\n",
      "  time_total_s: 105.52285099029541\n",
      "  timestamp: 1650226127\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00006\n",
      "  warmup_time: 0.0029397010803222656\n",
      "  \n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.3061\n",
      "  date: 2022-04-17_15-08-49\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.8974984268665314\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 110.91125154495239\n",
      "  time_this_iter_s: 110.91125154495239\n",
      "  time_total_s: 110.91125154495239\n",
      "  timestamp: 1650226129\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "Result for train_cifar_eeb8a_00001:\n",
      "  accuracy: 0.101\n",
      "  date: 2022-04-17_15-08-50\n",
      "  done: true\n",
      "  experiment_id: 622c9c9e401b45acb6928a0814bfc75f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.33481670126915\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10449\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 108.9163773059845\n",
      "  time_this_iter_s: 108.9163773059845\n",
      "  time_total_s: 108.9163773059845\n",
      "  timestamp: 1650226130\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: eeb8a_00001\n",
      "  warmup_time: 0.002600431442260742\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [2, 10000] loss: 0.273\n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.5292\n",
      "  date: 2022-04-17_15-08-51\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 3\n",
      "  loss: 1.3070130706310272\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 110.3855333328247\n",
      "  time_this_iter_s: 34.742993116378784\n",
      "  time_total_s: 110.3855333328247\n",
      "  timestamp: 1650226131\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:51 (running for 00:01:55.13)\n",
      "Memory usage on this node: 8.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.9199252038955688 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.30701 |     0.5292 |                    3 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.59997 |     0.4165 |                    4 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 2.00684 |     0.2586 |                    4 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.3003\n",
      "  date: 2022-04-17_15-08-52\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 5\n",
      "  loss: 1.8250634290695191\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 110.61765551567078\n",
      "  time_this_iter_s: 21.306682348251343\n",
      "  time_total_s: 110.61765551567078\n",
      "  timestamp: 1650226132\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [5,  2000] loss: 1.564\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:08:57 (running for 00:02:00.55)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.9199252038955688 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.57517 |     0.4436 |                    1 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.30701 |     0.5292 |                    3 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.59997 |     0.4165 |                    4 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.82506 |     0.3003 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.506\n",
      "  date: 2022-04-17_15-08-57\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.3788130595684052\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 116.1480553150177\n",
      "  time_this_iter_s: 54.42921805381775\n",
      "  time_total_s: 116.1480553150177\n",
      "  timestamp: 1650226137\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2,  2000] loss: 1.893\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:02 (running for 00:02:05.70)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.30701 |     0.5292 |                    3 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.59997 |     0.4165 |                    4 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.82506 |     0.3003 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [4,  2000] loss: 1.241\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.4442\n",
      "  date: 2022-04-17_15-09-03\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 5\n",
      "  loss: 1.5167266765594483\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 121.53389978408813\n",
      "  time_this_iter_s: 22.539472818374634\n",
      "  time_total_s: 121.53389978408813\n",
      "  timestamp: 1650226143\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [6,  2000] loss: 1.770\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [3,  2000] loss: 1.298\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2,  4000] loss: 0.913\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:08 (running for 00:02:11.36)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.30701 |     0.5292 |                    3 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.51673 |     0.4442 |                    5 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.82506 |     0.3003 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.3519\n",
      "  date: 2022-04-17_15-09-11\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 6\n",
      "  loss: 1.708685962677002\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 129.9303596019745\n",
      "  time_this_iter_s: 19.31270408630371\n",
      "  time_total_s: 129.9303596019745\n",
      "  timestamp: 1650226151\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [4,  4000] loss: 0.607\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [3,  4000] loss: 0.639\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2,  6000] loss: 0.603\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:16 (running for 00:02:19.84)\n",
      "Memory usage on this node: 9.0/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.30701 |     0.5292 |                    3 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.51673 |     0.4442 |                    5 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.70869 |     0.3519 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [6,  2000] loss: 1.494\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:21 (running for 00:02:24.85)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.803407021522522 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.30701 |     0.5292 |                    3 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.51673 |     0.4442 |                    5 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.70869 |     0.3519 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2,  8000] loss: 0.444\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [3,  6000] loss: 0.427\n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.5536\n",
      "  date: 2022-04-17_15-09-23\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.2497398596286773\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 141.87750697135925\n",
      "  time_this_iter_s: 31.491973638534546\n",
      "  time_total_s: 141.87750697135925\n",
      "  timestamp: 1650226163\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.4649\n",
      "  date: 2022-04-17_15-09-24\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 6\n",
      "  loss: 1.4633207991600037\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 142.88618540763855\n",
      "  time_this_iter_s: 21.352285623550415\n",
      "  time_total_s: 142.88618540763855\n",
      "  timestamp: 1650226164\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [7,  2000] loss: 1.660\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:29 (running for 00:02:32.71)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.24974 |     0.5536 |                    4 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.46332 |     0.4649 |                    6 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.70869 |     0.3519 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.3882\n",
      "  date: 2022-04-17_15-09-30\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 7\n",
      "  loss: 1.6194402955055236\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 149.15781426429749\n",
      "  time_this_iter_s: 19.227454662322998\n",
      "  time_total_s: 149.15781426429749\n",
      "  timestamp: 1650226170\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2, 10000] loss: 0.348\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [3,  8000] loss: 0.315\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [5,  2000] loss: 1.139\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:35 (running for 00:02:39.08)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.24974 |     0.5536 |                    4 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.46332 |     0.4649 |                    6 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.61944 |     0.3882 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [7,  2000] loss: 1.444\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2, 12000] loss: 0.282\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [3, 10000] loss: 0.255\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:40 (running for 00:02:44.09)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.37881 |     0.506  |                    2 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.24974 |     0.5536 |                    4 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.46332 |     0.4649 |                    6 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.61944 |     0.3882 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [8,  2000] loss: 1.595\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [5,  4000] loss: 0.573\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.485\n",
      "  date: 2022-04-17_15-09-45\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 7\n",
      "  loss: 1.4121691098213196\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 164.17867851257324\n",
      "  time_this_iter_s: 21.292493104934692\n",
      "  time_total_s: 164.17867851257324\n",
      "  timestamp: 1650226185\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5541\n",
      "  date: 2022-04-17_15-09-46\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 3\n",
      "  loss: 1.2715955495238305\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 165.31364464759827\n",
      "  time_this_iter_s: 49.165589332580566\n",
      "  time_total_s: 165.31364464759827\n",
      "  timestamp: 1650226186\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:46 (running for 00:02:49.87)\n",
      "Memory usage on this node: 8.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.24974 |     0.5536 |                    4 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.41217 |     0.485  |                    7 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.61944 |     0.3882 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2, 14000] loss: 0.240\n",
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.3997\n",
      "  date: 2022-04-17_15-09-50\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 8\n",
      "  loss: 1.5819545240402222\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 168.47115397453308\n",
      "  time_this_iter_s: 19.313339710235596\n",
      "  time_total_s: 168.47115397453308\n",
      "  timestamp: 1650226190\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.5916\n",
      "  date: 2022-04-17_15-09-54\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 5\n",
      "  loss: 1.156211072564125\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 173.34090113639832\n",
      "  time_this_iter_s: 31.463394165039062\n",
      "  time_total_s: 173.34090113639832\n",
      "  timestamp: 1650226194\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:54 (running for 00:02:58.09)\n",
      "Memory usage on this node: 8.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -1.5819545240402222 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.15621 |     0.5916 |                    5 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.41217 |     0.485  |                    7 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.58195 |     0.3997 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [4,  2000] loss: 1.195\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2, 16000] loss: 0.208\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:09:59 (running for 00:03:03.11)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -1.5819545240402222 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.15621 |     0.5916 |                    5 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.41217 |     0.485  |                    7 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.58195 |     0.3997 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [8,  2000] loss: 1.401\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [9,  2000] loss: 1.552\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [4,  4000] loss: 0.599\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2, 18000] loss: 0.179\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:04 (running for 00:03:08.12)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -1.5819545240402222 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.15621 |     0.5916 |                    5 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.41217 |     0.485  |                    7 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.58195 |     0.3997 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [6,  2000] loss: 1.073\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.4973\n",
      "  date: 2022-04-17_15-10-07\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 8\n",
      "  loss: 1.3856668543815613\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 185.68390655517578\n",
      "  time_this_iter_s: 21.50522804260254\n",
      "  time_total_s: 185.68390655517578\n",
      "  timestamp: 1650226207\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.4134\n",
      "  date: 2022-04-17_15-10-09\n",
      "  done: false\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 9\n",
      "  loss: 1.5523516991615296\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 187.6812460422516\n",
      "  time_this_iter_s: 19.210092067718506\n",
      "  time_total_s: 187.6812460422516\n",
      "  timestamp: 1650226209\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [4,  6000] loss: 0.406\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [2, 20000] loss: 0.163\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:14 (running for 00:03:17.60)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.15621 |     0.5916 |                    5 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.38567 |     0.4973 |                    8 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.55235 |     0.4134 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [6,  4000] loss: 0.536\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:19 (running for 00:03:22.61)\n",
      "Memory usage on this node: 8.9/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.6673567510366438 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.8975  |     0.3061 |                    1 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.15621 |     0.5916 |                    5 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.38567 |     0.4973 |                    8 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.55235 |     0.4134 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [4,  8000] loss: 0.304\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [9,  2000] loss: 1.355\n",
      "\u001b[2m\u001b[36m(func pid=10463)\u001b[0m [10,  2000] loss: 1.519\n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.4062\n",
      "  date: 2022-04-17_15-10-24\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.605290038704872\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 205.45410060882568\n",
      "  time_this_iter_s: 94.54284906387329\n",
      "  time_total_s: 205.45410060882568\n",
      "  timestamp: 1650226224\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.5791\n",
      "  date: 2022-04-17_15-10-26\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 6\n",
      "  loss: 1.1972561856925488\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 204.98265504837036\n",
      "  time_this_iter_s: 31.641753911972046\n",
      "  time_total_s: 204.98265504837036\n",
      "  timestamp: 1650226226\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:26 (running for 00:03:29.75)\n",
      "Memory usage on this node: 8.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 5.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.19726 |     0.5791 |                    6 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.38567 |     0.4973 |                    8 |\n",
      "| train_cifar_eeb8a_00008 | RUNNING    | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.55235 |     0.4134 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00008:\n",
      "  accuracy: 0.4237\n",
      "  date: 2022-04-17_15-10-28\n",
      "  done: true\n",
      "  experiment_id: 78d4db83c03e4b8e9793530158b0eb1a\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 10\n",
      "  loss: 1.517293008518219\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10463\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 207.07780957221985\n",
      "  time_this_iter_s: 19.39656352996826\n",
      "  time_total_s: 207.07780957221985\n",
      "  timestamp: 1650226228\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: eeb8a_00008\n",
      "  warmup_time: 0.0031423568725585938\n",
      "  \n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.5138\n",
      "  date: 2022-04-17_15-10-28\n",
      "  done: false\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 9\n",
      "  loss: 1.3439842971801759\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 207.29913663864136\n",
      "  time_this_iter_s: 21.615230083465576\n",
      "  time_total_s: 207.29913663864136\n",
      "  timestamp: 1650226228\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [4, 10000] loss: 0.243\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3,  2000] loss: 1.609\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:33 (running for 00:03:37.12)\n",
      "Memory usage on this node: 8.3/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.5999722444534301 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 4.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2716  |     0.5541 |                    3 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.19726 |     0.5791 |                    6 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.34398 |     0.5138 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5806\n",
      "  date: 2022-04-17_15-10-35\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.2054800664514302\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 214.33138298988342\n",
      "  time_this_iter_s: 49.017738342285156\n",
      "  time_total_s: 214.33138298988342\n",
      "  timestamp: 1650226235\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [7,  2000] loss: 1.011\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3,  4000] loss: 0.783\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:40 (running for 00:03:43.90)\n",
      "Memory usage on this node: 8.4/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 4.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.19726 |     0.5791 |                    6 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.34398 |     0.5138 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10461)\u001b[0m [10,  2000] loss: 1.314\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [5,  2000] loss: 1.124\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:45 (running for 00:03:48.91)\n",
      "Memory usage on this node: 8.4/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 4.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.19726 |     0.5791 |                    6 |\n",
      "| train_cifar_eeb8a_00007 | RUNNING    | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.34398 |     0.5138 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [7,  4000] loss: 0.510\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3,  6000] loss: 0.524\n",
      "Result for train_cifar_eeb8a_00007:\n",
      "  accuracy: 0.5263\n",
      "  date: 2022-04-17_15-10-48\n",
      "  done: true\n",
      "  experiment_id: 9817d6d08f0c43ad8ec31d3017d990a5\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 10\n",
      "  loss: 1.3164420022010803\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10461\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 227.2566545009613\n",
      "  time_this_iter_s: 19.957517862319946\n",
      "  time_total_s: 227.2566545009613\n",
      "  timestamp: 1650226248\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: eeb8a_00007\n",
      "  warmup_time: 0.0036857128143310547\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [5,  4000] loss: 0.578\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:53 (running for 00:03:57.09)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.19726 |     0.5791 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3,  8000] loss: 0.390\n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.5999\n",
      "  date: 2022-04-17_15-10-54\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 7\n",
      "  loss: 1.1476097927093505\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 233.45597124099731\n",
      "  time_this_iter_s: 28.473316192626953\n",
      "  time_total_s: 233.45597124099731\n",
      "  timestamp: 1650226254\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [5,  6000] loss: 0.385\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:10:59 (running for 00:04:03.21)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.14761 |     0.5999 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3, 10000] loss: 0.307\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [8,  2000] loss: 0.962\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:04 (running for 00:04:08.22)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.14761 |     0.5999 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [5,  8000] loss: 0.293\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3, 12000] loss: 0.257\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:09 (running for 00:04:13.22)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.14761 |     0.5999 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [5, 10000] loss: 0.231\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [8,  4000] loss: 0.478\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3, 14000] loss: 0.214\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:14 (running for 00:04:18.23)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.4838106892108918 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.20548 |     0.5806 |                    4 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.14761 |     0.5999 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5738\n",
      "  date: 2022-04-17_15-11-17\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 5\n",
      "  loss: 1.2240270031839608\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 256.74146938323975\n",
      "  time_this_iter_s: 42.41008639335632\n",
      "  time_total_s: 256.74146938323975\n",
      "  timestamp: 1650226277\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.6096\n",
      "  date: 2022-04-17_15-11-21\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 8\n",
      "  loss: 1.1286136580944062\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 259.784841299057\n",
      "  time_this_iter_s: 26.328870058059692\n",
      "  time_total_s: 259.784841299057\n",
      "  timestamp: 1650226281\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:21 (running for 00:04:24.54)\n",
      "Memory usage on this node: 7.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.12861 |     0.6096 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3, 16000] loss: 0.190\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [6,  2000] loss: 1.073\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:26 (running for 00:04:29.55)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.12861 |     0.6096 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3, 18000] loss: 0.165\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [9,  2000] loss: 0.890\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:31 (running for 00:04:34.56)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.12861 |     0.6096 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [6,  4000] loss: 0.558\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [3, 20000] loss: 0.148\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:36 (running for 00:04:39.56)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.12861 |     0.6096 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [9,  4000] loss: 0.455\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [6,  6000] loss: 0.369\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:41 (running for 00:04:44.57)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.60529 |     0.4062 |                    2 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.12861 |     0.6096 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.4809\n",
      "  date: 2022-04-17_15-11-45\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 3\n",
      "  loss: 1.4596245722293855\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 286.55428194999695\n",
      "  time_this_iter_s: 81.10018134117126\n",
      "  time_total_s: 286.55428194999695\n",
      "  timestamp: 1650226305\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [6,  8000] loss: 0.282\n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.631\n",
      "  date: 2022-04-17_15-11-47\n",
      "  done: false\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 9\n",
      "  loss: 1.0585486007094382\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 286.2876989841461\n",
      "  time_this_iter_s: 26.50285768508911\n",
      "  time_total_s: 286.2876989841461\n",
      "  timestamp: 1650226307\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:47 (running for 00:04:51.05)\n",
      "Memory usage on this node: 7.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.05855 |     0.631  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4,  2000] loss: 1.449\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:52 (running for 00:04:56.05)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.05855 |     0.631  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [6, 10000] loss: 0.225\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [10,  2000] loss: 0.852\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:11:57 (running for 00:05:01.07)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.22403 |     0.5738 |                    5 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.05855 |     0.631  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4,  4000] loss: 0.717\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5698\n",
      "  date: 2022-04-17_15-11-59\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 6\n",
      "  loss: 1.2495471814572812\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 298.6583905220032\n",
      "  time_this_iter_s: 41.91692113876343\n",
      "  time_total_s: 298.6583905220032\n",
      "  timestamp: 1650226319\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:04 (running for 00:05:08.21)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.05855 |     0.631  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4,  6000] loss: 0.475\n",
      "\u001b[2m\u001b[36m(func pid=10457)\u001b[0m [10,  4000] loss: 0.422\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [7,  2000] loss: 1.048\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:09 (running for 00:05:13.22)\n",
      "Memory usage on this node: 7.8/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 3.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00005 | RUNNING    | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.05855 |     0.631  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4,  8000] loss: 0.357\n",
      "Result for train_cifar_eeb8a_00005:\n",
      "  accuracy: 0.6314\n",
      "  date: 2022-04-17_15-12-14\n",
      "  done: true\n",
      "  experiment_id: 57a10d764cea4801b1a7053d91aae46f\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 10\n",
      "  loss: 1.0740672340273858\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10457\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 312.6447048187256\n",
      "  time_this_iter_s: 26.357005834579468\n",
      "  time_total_s: 312.6447048187256\n",
      "  timestamp: 1650226334\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: eeb8a_00005\n",
      "  warmup_time: 0.0038216114044189453\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [7,  4000] loss: 0.532\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:19 (running for 00:05:22.40)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4, 10000] loss: 0.280\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [7,  6000] loss: 0.362\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:24 (running for 00:05:27.42)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4, 12000] loss: 0.234\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [7,  8000] loss: 0.276\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:29 (running for 00:05:32.42)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4, 14000] loss: 0.200\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:34 (running for 00:05:37.43)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [7, 10000] loss: 0.220\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4, 16000] loss: 0.173\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:39 (running for 00:05:42.44)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.24955 |     0.5698 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5738\n",
      "  date: 2022-04-17_15-12-39\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 7\n",
      "  loss: 1.2426046735525131\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 338.29783844947815\n",
      "  time_this_iter_s: 39.639447927474976\n",
      "  time_total_s: 338.29783844947815\n",
      "  timestamp: 1650226359\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:44 (running for 00:05:47.86)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4, 18000] loss: 0.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [8,  2000] loss: 1.016\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:49 (running for 00:05:52.86)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [4, 20000] loss: 0.136\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [8,  4000] loss: 0.511\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:54 (running for 00:05:57.87)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:12:59 (running for 00:06:02.88)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.4248560520410538 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.45962 |     0.4809 |                    3 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [8,  6000] loss: 0.352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.5104\n",
      "  date: 2022-04-17_15-13-00\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.3481265175111592\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 361.7508692741394\n",
      "  time_this_iter_s: 75.19658732414246\n",
      "  time_total_s: 361.7508692741394\n",
      "  timestamp: 1650226380\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:05 (running for 00:06:08.79)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [8,  8000] loss: 0.274\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5,  2000] loss: 1.333\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:10 (running for 00:06:13.79)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5,  4000] loss: 0.670\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [8, 10000] loss: 0.215\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:15 (running for 00:06:18.80)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.3856668543815613 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.2426  |     0.5738 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5879\n",
      "  date: 2022-04-17_15-13-18\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 8\n",
      "  loss: 1.2114240280002355\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 377.0315110683441\n",
      "  time_this_iter_s: 38.73367261886597\n",
      "  time_total_s: 377.0315110683441\n",
      "  timestamp: 1650226398\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5,  6000] loss: 0.454\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:23 (running for 00:06:26.58)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [9,  2000] loss: 0.996\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5,  8000] loss: 0.329\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:28 (running for 00:06:31.60)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [9,  4000] loss: 0.509\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5, 10000] loss: 0.269\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:33 (running for 00:06:36.60)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:38 (running for 00:06:41.61)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5, 12000] loss: 0.217\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [9,  6000] loss: 0.341\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:43 (running for 00:06:46.62)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5, 14000] loss: 0.187\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [9,  8000] loss: 0.260\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:48 (running for 00:06:51.63)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5, 16000] loss: 0.163\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [9, 10000] loss: 0.212\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:13:53 (running for 00:06:56.64)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.21142 |     0.5879 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.582\n",
      "  date: 2022-04-17_15-13-57\n",
      "  done: false\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 9\n",
      "  loss: 1.2594054866731166\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 416.04824805259705\n",
      "  time_this_iter_s: 39.01673698425293\n",
      "  time_total_s: 416.04824805259705\n",
      "  timestamp: 1650226437\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5, 18000] loss: 0.146\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:02 (running for 00:07:05.60)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [5, 20000] loss: 0.126\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [10,  2000] loss: 0.960\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:07 (running for 00:07:10.61)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [10,  4000] loss: 0.500\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:12 (running for 00:07:15.62)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.34813 |     0.5104 |                    4 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.5323\n",
      "  date: 2022-04-17_15-14-13\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 5\n",
      "  loss: 1.2955285732276738\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 434.84076595306396\n",
      "  time_this_iter_s: 73.08989667892456\n",
      "  time_total_s: 434.84076595306396\n",
      "  timestamp: 1650226453\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [10,  6000] loss: 0.340\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:18 (running for 00:07:21.87)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6,  2000] loss: 1.262\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:23 (running for 00:07:26.89)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [10,  8000] loss: 0.258\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6,  4000] loss: 0.632\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:28 (running for 00:07:31.89)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10451)\u001b[0m [10, 10000] loss: 0.207\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6,  6000] loss: 0.411\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:33 (running for 00:07:36.90)\n",
      "Memory usage on this node: 7.2/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 2.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00002 | RUNNING    | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.25941 |     0.582  |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00002:\n",
      "  accuracy: 0.5814\n",
      "  date: 2022-04-17_15-14-36\n",
      "  done: true\n",
      "  experiment_id: 0f715c3035174dbeac144ea84e75d74d\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 10\n",
      "  loss: 1.2360811105832457\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10451\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 455.3443212509155\n",
      "  time_this_iter_s: 39.29607319831848\n",
      "  time_total_s: 455.3443212509155\n",
      "  timestamp: 1650226476\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: eeb8a_00002\n",
      "  warmup_time: 0.002669811248779297\n",
      "  \n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6,  8000] loss: 0.322\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:41 (running for 00:07:44.91)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6, 10000] loss: 0.250\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:46 (running for 00:07:49.92)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6, 12000] loss: 0.205\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:51 (running for 00:07:54.93)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:14:56 (running for 00:07:59.94)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6, 14000] loss: 0.177\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:01 (running for 00:08:04.94)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6, 16000] loss: 0.153\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:06 (running for 00:08:09.95)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6, 18000] loss: 0.137\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:11 (running for 00:08:14.96)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [6, 20000] loss: 0.125\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:16 (running for 00:08:19.97)\n",
      "Memory usage on this node: 6.6/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:21 (running for 00:08:24.98)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.29553 |     0.5323 |                    5 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.5477\n",
      "  date: 2022-04-17_15-15-23\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 6\n",
      "  loss: 1.2722323699684814\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 505.1507694721222\n",
      "  time_this_iter_s: 70.31000351905823\n",
      "  time_total_s: 505.1507694721222\n",
      "  timestamp: 1650226523\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:28 (running for 00:08:32.19)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7,  2000] loss: 1.198\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:33 (running for 00:08:37.20)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7,  4000] loss: 0.593\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:38 (running for 00:08:42.21)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7,  6000] loss: 0.401\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:43 (running for 00:08:47.21)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7,  8000] loss: 0.296\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:48 (running for 00:08:52.23)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7, 10000] loss: 0.235\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:53 (running for 00:08:57.23)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:15:58 (running for 00:09:02.25)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7, 12000] loss: 0.196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:03 (running for 00:09:07.25)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7, 14000] loss: 0.174\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:08 (running for 00:09:12.26)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7, 16000] loss: 0.149\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:13 (running for 00:09:17.27)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7, 18000] loss: 0.129\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:18 (running for 00:09:22.28)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [7, 20000] loss: 0.121\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:23 (running for 00:09:27.29)\n",
      "Memory usage on this node: 6.6/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:28 (running for 00:09:32.30)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.27223 |     0.5477 |                    6 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.5625\n",
      "  date: 2022-04-17_15-16-31\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 7\n",
      "  loss: 1.2451350928487257\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 573.0253887176514\n",
      "  time_this_iter_s: 67.87461924552917\n",
      "  time_total_s: 573.0253887176514\n",
      "  timestamp: 1650226591\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:36 (running for 00:09:40.06)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8,  2000] loss: 1.133\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:41 (running for 00:09:45.07)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8,  4000] loss: 0.571\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:46 (running for 00:09:50.07)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8,  6000] loss: 0.382\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:51 (running for 00:09:55.09)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8,  8000] loss: 0.286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:16:56 (running for 00:10:00.09)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8, 10000] loss: 0.228\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:01 (running for 00:10:05.10)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:06 (running for 00:10:10.11)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8, 12000] loss: 0.193\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:11 (running for 00:10:15.12)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8, 14000] loss: 0.163\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:16 (running for 00:10:20.13)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8, 16000] loss: 0.141\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:21 (running for 00:10:25.14)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8, 18000] loss: 0.125\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:26 (running for 00:10:30.14)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [8, 20000] loss: 0.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:31 (running for 00:10:35.16)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:36 (running for 00:10:40.16)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2985454411908983 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.24514 |     0.5625 |                    7 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.5762\n",
      "  date: 2022-04-17_15-17-39\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 8\n",
      "  loss: 1.1793832218278666\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 640.5711138248444\n",
      "  time_this_iter_s: 67.545725107193\n",
      "  time_total_s: 640.5711138248444\n",
      "  timestamp: 1650226659\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:44 (running for 00:10:47.61)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9,  2000] loss: 1.066\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:49 (running for 00:10:52.62)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9,  4000] loss: 0.547\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:54 (running for 00:10:57.63)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9,  6000] loss: 0.367\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:17:59 (running for 00:11:02.64)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9,  8000] loss: 0.271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:04 (running for 00:11:07.65)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9, 10000] loss: 0.216\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:09 (running for 00:11:12.65)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:14 (running for 00:11:17.66)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9, 12000] loss: 0.183\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:19 (running for 00:11:22.67)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9, 14000] loss: 0.156\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:24 (running for 00:11:27.68)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9, 16000] loss: 0.138\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:29 (running for 00:11:32.69)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9, 18000] loss: 0.125\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:34 (running for 00:11:37.70)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [9, 20000] loss: 0.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:39 (running for 00:11:42.70)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:44 (running for 00:11:47.71)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.17938 |     0.5762 |                    8 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.5961\n",
      "  date: 2022-04-17_15-18-47\n",
      "  done: false\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 9\n",
      "  loss: 1.1391837363489439\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 708.5163178443909\n",
      "  time_this_iter_s: 67.94520401954651\n",
      "  time_total_s: 708.5163178443909\n",
      "  timestamp: 1650226727\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:52 (running for 00:11:55.55)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10,  2000] loss: 1.045\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:18:57 (running for 00:12:00.56)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10,  4000] loss: 0.527\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:02 (running for 00:12:05.57)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10,  6000] loss: 0.349\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:07 (running for 00:12:10.58)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10,  8000] loss: 0.258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:12 (running for 00:12:15.59)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10, 10000] loss: 0.211\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:17 (running for 00:12:20.60)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10, 12000] loss: 0.176\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:22 (running for 00:12:25.60)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10, 14000] loss: 0.151\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:27 (running for 00:12:30.61)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:32 (running for 00:12:35.62)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10, 16000] loss: 0.131\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:37 (running for 00:12:40.63)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10, 18000] loss: 0.118\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:42 (running for 00:12:45.64)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=10421)\u001b[0m [10, 20000] loss: 0.105\n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:47 (running for 00:12:50.65)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:52 (running for 00:12:55.66)\n",
      "Memory usage on this node: 6.7/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 1.0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | RUNNING    | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.13918 |     0.5961 |                    9 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 15:19:53,963\tINFO tune.py:701 -- Total run time: 777.33 seconds (777.16 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_cifar_eeb8a_00000:\n",
      "  accuracy: 0.613\n",
      "  date: 2022-04-17_15-19-53\n",
      "  done: true\n",
      "  experiment_id: 5feb8025c165493a984a69220e21f068\n",
      "  hostname: workstation\n",
      "  iterations_since_restore: 10\n",
      "  loss: 1.0997307363914326\n",
      "  node_ip: 192.168.1.108\n",
      "  pid: 10421\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 775.132691860199\n",
      "  time_this_iter_s: 66.6163740158081\n",
      "  time_total_s: 775.132691860199\n",
      "  timestamp: 1650226793\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: eeb8a_00000\n",
      "  warmup_time: 0.0039730072021484375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-17 15:19:53 (running for 00:12:57.17)\n",
      "Memory usage on this node: 6.3/62.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -1.2114240280002355 | Iter 4.000: -1.3481265175111592 | Iter 2.000: -1.605290038704872 | Iter 1.000: -2.28803605966568\n",
      "Resources requested: 0/16 CPUs, 0/2 GPUs, 0.0/34.71 GiB heap, 0.0/17.35 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/priyammazumdar/ray_results/train_cifar_2022-04-17_15-06-56\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status     | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_eeb8a_00000 | TERMINATED | 192.168.1.108:10421 |            2 |  128 |   32 | 0.000117108 | 1.09973 |     0.613  |                   10 |\n",
      "| train_cifar_eeb8a_00001 | TERMINATED | 192.168.1.108:10449 |            2 |   32 |    4 | 0.0240294   | 2.33482 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00002 | TERMINATED | 192.168.1.108:10451 |            4 |   64 |   32 | 0.00156282  | 1.23608 |     0.5814 |                   10 |\n",
      "| train_cifar_eeb8a_00003 | TERMINATED | 192.168.1.108:10453 |            8 |    4 |    8 | 0.0923667   | 2.33653 |     0.0997 |                    1 |\n",
      "| train_cifar_eeb8a_00004 | TERMINATED | 192.168.1.108:10454 |            4 |   64 |    4 | 0.0155539   | 2.30449 |     0.101  |                    1 |\n",
      "| train_cifar_eeb8a_00005 | TERMINATED | 192.168.1.108:10457 |            8 |  128 |  256 | 0.000899072 | 1.07407 |     0.6314 |                   10 |\n",
      "| train_cifar_eeb8a_00006 | TERMINATED | 192.168.1.108:10459 |            2 |    4 |    8 | 0.045046    | 2.41185 |     0.0977 |                    1 |\n",
      "| train_cifar_eeb8a_00007 | TERMINATED | 192.168.1.108:10461 |           16 |  256 |  128 | 0.000332309 | 1.31644 |     0.5263 |                   10 |\n",
      "| train_cifar_eeb8a_00008 | TERMINATED | 192.168.1.108:10463 |           16 |    4 |  128 | 0.000234467 | 1.51729 |     0.4237 |                   10 |\n",
      "| train_cifar_eeb8a_00009 | TERMINATED | 192.168.1.108:10465 |            4 |  256 |   32 | 0.000128987 | 2.27377 |     0.1508 |                    1 |\n",
      "+-------------------------+------------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "Best trial config: {'l1': 128, 'l2': 256, 'lr': 0.0008990724724802694, 'batch_size': 8}\n",
      "Best trial final validation loss: 1.0740672340273858\n",
      "Best trial final validation accuracy: 0.6314\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best trial test set accuracy: 0.6384\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "\n",
    "# You can change the number of GPUs per trial here:\n",
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
